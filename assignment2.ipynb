{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics for Unstructured Data Assignment 2\n",
    "## Building a Crowdsourced Recommender System\n",
    "\n",
    "Team members:\n",
    "- Marcus Martinez\n",
    "- Marifer Martinez-Garcia\n",
    "- Femke Munting\n",
    "- Alex Schmelzeis\n",
    "- Milan Vaghani\n",
    "- Kennedy Zapalac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pip install undected_chromedriver #run this if you are using it for the first time\n",
    "# Importing modules related to web automation (Selenium)\n",
    "#from selenium import webdriver\n",
    "#from selenium.webdriver.common.keys import Keys\n",
    "#from selenium.webdriver.common.by import By\n",
    "#from selenium.webdriver.support.ui import WebDriverWait\n",
    "#from selenium.webdriver.support import expected_conditions as EC\n",
    "#from selenium.webdriver.common.action_chains import ActionChains\n",
    "#from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "# Uncomment the following if using undetected_chromedriver for scraping\n",
    "# import undetected_chromedriver as uc\n",
    "\n",
    "# Importing standard libraries\n",
    "import time\n",
    "import string\n",
    "import io\n",
    "import csv\n",
    "import re\n",
    "import shutil\n",
    "import random\n",
    "from tempfile import NamedTemporaryFile\n",
    "import decimal\n",
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "from itertools import combinations\n",
    "import operator\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning and manifold learning\n",
    "from sklearn import manifold\n",
    "\n",
    "# Natural language processing (NLP)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Machine learning and text processing\n",
    "from sklearn import manifold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping\n",
    "We have commented out the scraping code for now, but you can run it if you remove the comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for Beer Advocate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the undetected Chrome driver with options\n",
    "# options = uc.ChromeOptions()\n",
    "# options.add_argument('--no-sandbox')\n",
    "# options.add_argument('--disable-dev-shm-usage')\n",
    "# options.add_argument('--window-size=1920,1080')\n",
    "# options.add_argument('--disable-blink-features=AutomationControlled')  # Helps avoid detection\n",
    "\n",
    "# # Initialize the Chrome WebDriver using undetected-chromedriver\n",
    "# driver = uc.Chrome(options=options)\n",
    "\n",
    "# # Open the BeerAdvocate top-rated beers page\n",
    "# driver.get('https://www.beeradvocate.com/beer/top-rated/')\n",
    "\n",
    "# wait = WebDriverWait(driver, 10)\n",
    "\n",
    "# # Dictionary to store beer data\n",
    "# beer_data = {}\n",
    "\n",
    "# # Loop through each beer listed on the page (250 beers)\n",
    "# for i in range(2, 252):  # Beer list starts from row 2 and goes up to 251\n",
    "#     try:\n",
    "#         # Locate the beer name and URL\n",
    "#         beer_link = wait.until(EC.presence_of_element_located(\n",
    "#             (By.XPATH, f'//div[@id=\"ba-content\"]/table/tbody/tr[{i}]/td[2]/a')))\n",
    "#         beer_name = beer_link.text\n",
    "#         beer_url = beer_link.get_attribute('href')\n",
    "#         # Store the name and URL in the dictionary\n",
    "#         beer_data[beer_name] = beer_url\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in processing beer {i}: {e}\")\n",
    "#         continue\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the undetected Chrome driver with options\n",
    "# options = uc.ChromeOptions()\n",
    "# options.add_argument('--no-sandbox')\n",
    "# options.add_argument('--disable-dev-shm-usage')\n",
    "# options.add_argument('--window-size=1920,1080')\n",
    "# options.add_argument('--disable-blink-features=AutomationControlled')  # Helps avoid detection\n",
    "\n",
    "# # Initialize the Chrome WebDriver using undetected-chromedriver\n",
    "# driver = uc.Chrome(options=options)\n",
    "# # Open the first beer's page\n",
    "# comments_file = pd.DataFrame( columns = ['Beer','Rating','Review'])\n",
    "\n",
    "# for key,value in beer_data.items():\n",
    "#         driver.get(value)\n",
    "#         reviews = driver.find_elements(By.XPATH,'//div[@id=\"rating_fullview_content_2\"]')\n",
    "#         for review in reviews[:25]:\n",
    "#             try:\n",
    "#                 review_cmmnts = rep5 = review.find_element(By.XPATH,'.//div').text\n",
    "#                 rating = review.find_element(By.XPATH,'.//span[2]').text\n",
    "#                 review_cmmnts = review_cmmnts.replace('rDev',\"\")\n",
    "#                 k = pd.DataFrame({'Beer': [key], 'Rating': [rating], 'Review': [review_cmmnts]})\n",
    "#                 comments_file = pd.concat([comments_file, k], ignore_index=True)\n",
    "#             except:\n",
    "#                 pass\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for Rate Beer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize undetected Chrome driver with options\n",
    "# options = webdriver.ChromeOptions()\n",
    "# options.add_argument('--no-sandbox')\n",
    "# options.add_argument('--disable-dev-shm-usage')\n",
    "# options.add_argument('--window-size=1920,1080')\n",
    "# options.add_argument('--disable-blink-features=AutomationControlled')  # Helps avoid detection\n",
    "\n",
    "# # Initialize the Chrome WebDriver using undetected-chromedriver\n",
    "# driver = uc.Chrome(options=options)\n",
    "\n",
    "# # Open the BeerAdvocate top-rated beers page\n",
    "# driver.get('https://www.ratebeer.com/top-beers')\n",
    "\n",
    "# wait = WebDriverWait(driver, 10)\n",
    "\n",
    "# # Dictionary to store beer data\n",
    "# beer_data = {}\n",
    "\n",
    "# # Wait for the page to load (use dynamic waiting instead of time.sleep)\n",
    "# time.sleep(5)\n",
    "\n",
    "# # Loop through each beer listed on the page (usually 50-250 beers)\n",
    "# for i in range(1, 51):  # Adjust the range based on how many items are loaded\n",
    "#     try:\n",
    "#         # Locate the beer name and URL using XPath\n",
    "#         beer_link = wait.until(EC.presence_of_element_located(\n",
    "#             (By.XPATH, f'//*[@id=\"root\"]/div[2]/div[2]/div/div/div/div/div/section[2]/div[2]/div/div/div[2]/div[{i}]/div[2]/a/div[1]')))\n",
    "\n",
    "#         # Get the beer name\n",
    "#         beer_name = beer_link.text\n",
    "        \n",
    "#         # Get the beer URL\n",
    "#         beer_url = beer_link.find_element(By.XPATH, '..').get_attribute('href')\n",
    "\n",
    "#         # Store the name and URL in the dictionary\n",
    "#         beer_data[beer_name] = beer_url\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in processing beer {i}: {e}\")\n",
    "#         continue\n",
    "\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the undetected Chrome driver with options\n",
    "# options = uc.ChromeOptions()\n",
    "# options.add_argument('--no-sandbox')\n",
    "# options.add_argument('--disable-dev-shm-usage')\n",
    "# options.add_argument('--window-size=1920,1080')\n",
    "# options.add_argument('--disable-blink-features=AutomationControlled')  # Helps avoid detection\n",
    "\n",
    "# # Initialize the Chrome WebDriver using undetected-chromedriver\n",
    "# driver = uc.Chrome(options=options)\n",
    "\n",
    "# # Create a DataFrame to store beer reviews\n",
    "\n",
    "# # Iterate over the beer_data dictionary\n",
    "# for beer_name, beer_url in beer_data.items():\n",
    "#     driver.get(beer_url)\n",
    "    \n",
    "#     # Wait for the page to load\n",
    "#     time.sleep(2)  # You may adjust this wait time or implement a better waiting strategy\n",
    "\n",
    "#     # Loop through the first 15 reviews\n",
    "#     for i in range(1, 16):\n",
    "#         try:\n",
    "#             # Get the review comment\n",
    "#             review_xpath = f'//*[@id=\"root\"]/div[2]/div[2]/div/div/div/div[2]/div[1]/div[2]/div/div[3]/div[{i}]/div/section[2]/div[1]/div/div[1]/div'\n",
    "#             review_comment = driver.find_element(By.XPATH, review_xpath).text\n",
    "            \n",
    "#             # Get the rating\n",
    "#             rating_xpath = f'//*[@id=\"root\"]/div[2]/div[2]/div/div/div/div[2]/div[1]/div[2]/div/div[3]/div[{i}]/div/section[1]/div[2]/div[2]/div[2]/span[1]'\n",
    "#             rating = driver.find_element(By.XPATH, rating_xpath).text\n",
    "            \n",
    "#             # Store the data in the DataFrame\n",
    "#             k = pd.DataFrame({'Beer': [beer_name], 'Rating': [rating], 'Review': [review_comment]})\n",
    "#             comments_file = pd.concat([comments_file, k], ignore_index=True)\n",
    "\n",
    "#         except Exception as e:\n",
    "#             continue\n",
    "\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments_file.to_csv('data/beer_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Distribution of Review Counts')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/beer_reviews.csv\")\n",
    "\n",
    "reviews = df.groupby([\"Beer\"])[\"Review\"].count()\n",
    "# The max number of reviews for one beer is 15, the min is 1\n",
    "\n",
    "df.groupby([\"Beer\"])[\"Review\"].count().median()\n",
    "# Mean count is 6; maybe we should cut off all beers that have less than three reviews?\n",
    "\n",
    "# Display frequency of reviews\n",
    "\n",
    "sns.histplot(reviews, binwidth = 1)\n",
    "plt.title(\"Distribution of Review Counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_than_3 = df.groupby([\"Beer\"])[\"Review\"].count()\n",
    "more_than_3 = more_than_3[more_than_3 >= 5]\n",
    "more_than_3 = more_than_3.reset_index()\n",
    "\n",
    "reduced_df = df[df[\"Beer\"].isin(list(more_than_3[\"Beer\"]))]\n",
    "len(reduced_df[\"Beer\"].unique())\n",
    "reduced_df.to_csv(\"data/reduced_length.csv\", index = False)\n",
    "#len(reduced_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task B: Beer Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use raw word frequneices or post frequenices? Used post frequencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fmunting/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word frequencies written to data/word_freq.csv\n"
     ]
    }
   ],
   "source": [
    "# raw word frequencies\n",
    "# Download stopwords from the NLTK package\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Input and output filenames\n",
    "input_filename = 'data/reduced_length.csv'  # Input file\n",
    "final_filename = 'data/beer_reviews2.csv'  # Intermediate file without the column header\n",
    "word_freq_output = 'data/word_freq.csv'  # Output file for word frequencies\n",
    "\n",
    "# Function to clean and tokenize sentences\n",
    "def clean_and_tokenize(sentence):\n",
    "    \"\"\"\n",
    "    Cleans a given sentence by removing punctuation, converting text to lowercase,\n",
    "    and tokenizing the remaining words.\n",
    "    \"\"\"\n",
    "    # Remove punctuation and convert text to lowercase\n",
    "    sentence = re.sub(f'[{re.escape(string.punctuation)}]', '', sentence.lower())\n",
    "\n",
    "    # Tokenize and remove stopwords\n",
    "    return [word for word in sentence.split()]\n",
    "\n",
    "\n",
    "# Step 1: Extract and clean sentences from the text\n",
    "def extract_sentences(file):\n",
    "    \"\"\"\n",
    "    Extracts text data from the third column of the CSV file, splits it into sentences,\n",
    "    and cleans each sentence by removing punctuation and stopwords.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file)\n",
    "    posts = df.iloc[:,2]\n",
    "    sentences = []\n",
    "    sentences_clean = []\n",
    "    for post in posts:\n",
    "        sentences.extend(re.split('[?.!]', post))\n",
    "    for sentence in sentences:\n",
    "        cleaned_tokens = clean_and_tokenize(sentence)\n",
    "        if cleaned_tokens:\n",
    "            sentences_clean.append(cleaned_tokens)\n",
    "    return sentences_clean\n",
    "\n",
    "\n",
    "# Step 2: Calculate word frequencies\n",
    "def calculate_word_frequencies(sentences):\n",
    "    \"\"\"\n",
    "    Calculates the frequency of each word in the given list of cleaned sentences.\n",
    "    \"\"\"\n",
    "    freqs = {}\n",
    "    total_words = 0\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            freqs[word] = freqs.get(word, 0) + 1\n",
    "            total_words += 1\n",
    "    return freqs\n",
    "\n",
    "# Step 3: Write word frequencies to CSV\n",
    "def write_word_frequencies(word_freq, output_file):\n",
    "    \"\"\"\n",
    "    Writes the word frequencies to the specified CSV file.\n",
    "    \"\"\"\n",
    "    word_freq_df = pd.DataFrame(word_freq.items(), columns = [\"Word\", \"Frequency\"])\n",
    "    word_freq_df = word_freq_df.sort_values(by = \"Frequency\", ascending = False)\n",
    "    word_freq_df.to_csv(output_file, index=False)\n",
    "    print(f\"Word frequencies written to {output_file}\")\n",
    "\n",
    "# Main function to run all steps\n",
    "def main():\n",
    "    \n",
    "    # Step 1: Extract and clean sentences\n",
    "    sentences = extract_sentences(input_filename)\n",
    "    \n",
    "    # Step 2: Calculate word frequencies\n",
    "    word_freq = calculate_word_frequencies(sentences)\n",
    "    \n",
    "    # Step 3: Write word frequencies to CSV\n",
    "    write_word_frequencies(word_freq, word_freq_output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fmunting/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word post counts written to data/word_post_count.csv\n"
     ]
    }
   ],
   "source": [
    "# post word frequency\n",
    "# Download stopwords from the NLTK package\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Input and output filenames\n",
    "input_filename = 'data/reduced_length.csv'  # Input file\n",
    "word_freq_output = 'data/word_post_count.csv'  # Output file for word post count\n",
    "\n",
    "# Function to clean text (removing punctuation and stopwords)\n",
    "def clean_text(post):\n",
    "    \"\"\"\n",
    "    Cleans a given post by removing punctuation and stopwords, and converting text to lowercase.\n",
    "    \"\"\"\n",
    "    post = re.sub(f'[{re.escape(string.punctuation)}]', '', post.lower())  # Remove punctuation and convert to lowercase\n",
    "    words = post.split()  # Tokenize the post\n",
    "    words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Step 1: Clean the posts from the CSV\n",
    "def clean_posts(input_file):\n",
    "    \"\"\"\n",
    "    Reads the input CSV file, cleans the posts, and returns a list of cleaned posts.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_file)  # Load CSV\n",
    "    posts = df.iloc[:, 2]  # Extract the third column (posts)\n",
    "    cleaned_posts = posts.apply(clean_text)  # Apply cleaning to each post\n",
    "    return cleaned_posts\n",
    "\n",
    "# Step 2: Use CountVectorizer to count how many posts each word appears in\n",
    "def count_word_occurrences(posts):\n",
    "    \"\"\"\n",
    "    Uses CountVectorizer to count how many posts each word appears in.\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(binary=True)  # binary=True to count post occurrences (not word frequency)\n",
    "    X = vectorizer.fit_transform(posts)  # Transform the posts into a document-term matrix\n",
    "    word_counts = X.toarray().sum(axis=0)  # Sum the binary values to get the number of posts each word appears in\n",
    "    words = vectorizer.get_feature_names_out()  # Get the words from the vocabulary\n",
    "    word_post_count = dict(zip(words, word_counts))  # Map words to their post counts\n",
    "    return word_post_count\n",
    "\n",
    "# Step 3: Write the word counts to a CSV file\n",
    "def write_word_counts_to_csv(word_post_count, output_file):\n",
    "    \"\"\"\n",
    "    Writes the word-post-counts to the specified CSV file.\n",
    "    \"\"\"\n",
    "    word_post_count_df = pd.DataFrame(list(word_post_count.items()), columns=[\"Word\", \"Post_Count\"])\n",
    "    word_post_count_df = word_post_count_df.sort_values(by=\"Post_Count\", ascending=False)\n",
    "    word_post_count_df.to_csv(output_file, index=False)\n",
    "    print(f\"Word post counts written to {output_file}\")\n",
    "\n",
    "# Main function to run all steps\n",
    "def main():\n",
    "    # Step 1: Clean posts\n",
    "    cleaned_posts = clean_posts(input_filename)\n",
    "    \n",
    "    # Step 2: Count how many posts each word appears in\n",
    "    word_post_count = count_word_occurrences(cleaned_posts)\n",
    "    \n",
    "    # Step 3: Write word post counts to CSV\n",
    "    write_word_counts_to_csv(word_post_count, word_freq_output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Post_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>head</td>\n",
       "      <td>855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>taste</td>\n",
       "      <td>608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bottle</td>\n",
       "      <td>545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aroma</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>beer</td>\n",
       "      <td>475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9453</th>\n",
       "      <td>hangen</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9454</th>\n",
       "      <td>hang</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9455</th>\n",
       "      <td>handful</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9456</th>\n",
       "      <td>hamilton</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9457</th>\n",
       "      <td>ярко</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9458 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word  Post_Count\n",
       "0         head         855\n",
       "1        taste         608\n",
       "2       bottle         545\n",
       "3        aroma         513\n",
       "4         beer         475\n",
       "...        ...         ...\n",
       "9453    hangen           1\n",
       "9454      hang           1\n",
       "9455   handful           1\n",
       "9456  hamilton           1\n",
       "9457      ярко           1\n",
       "\n",
       "[9458 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/word_post_count.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Frequent beer qualities:**\n",
    "# NOTE: THIS NEEDS TO BE CHANGED NOW THAT WE HAVE THE NEW INPUT FILE\n",
    "1. Head: Refers to the foam on top of beer after it is poured, considered a quality aspect.\n",
    "2. Aroma?: Refers to the smell or scent of beer, often a key quality evaluated in tasting.\n",
    "3. Sweet\n",
    "4. Dark\n",
    "5. Pours?\n",
    "6. Chocolate\n",
    "7. Black\n",
    "8. Body?: Describes the mouthfeel or weight of the beer (light, medium, or full-bodied). Might be too general\n",
    "9. Vanilla\n",
    "10. Carbonation\n",
    "11. Light\n",
    "12. Smooth\n",
    "13. Fruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check that how attributes are used\n",
    "#df = pd.read_csv('data/beer_reviews.csv')\n",
    "#df_head = df.loc[df['Review'].str.contains(r'\\bhead\\b', case=False), 'Review']\n",
    "#for i in range(len(df_head)):\n",
    "#    print(df.loc[i, 'Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that 3 attributes chosen occur together\n",
    "# Initialize global variables and data structures\n",
    "df_lift = pd.DataFrame(columns=['word1', 'word2', 'lift_value'])  # To store lift values\n",
    "word_frequency = {}  # Dictionary to store word frequency in posts\n",
    "word_pair_frequency = defaultdict(dict)  # Dictionary to store word pair co-occurrence frequency\n",
    "results_dict = {}  # Dictionary to store results with lift values for word pairs\n",
    "file_length = 0  # Number of rows in the input file\n",
    "itr = 0  # Row iterator for the lift DataFrame\n",
    "\n",
    "# File paths\n",
    "input_file = 'data/reduced_length.csv'  # Input data file\n",
    "pair_keys_file = 'data/beer_attributes.txt'  # File containing the words to calculate lift\n",
    "output_lift_values = 'data/Lift_Values.csv'  # Output file for lift values\n",
    "output_lift_matrix = 'data/Lift_Matrix.csv'  # Output file for lift matrix\n",
    "\n",
    "# Load NLTK stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to clean and tokenize text (removes punctuation and stopwords)\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans a given text by removing punctuation, converting it to lowercase,\n",
    "    and tokenizing it, ignoring any stopwords.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    cleaned_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    return cleaned_tokens\n",
    "\n",
    "# Step 1: Load the words from the edmunds_pair_keys.txt file and generate all pairs\n",
    "def load_word_pairs(filename):\n",
    "    \"\"\"\n",
    "    Loads words from a file where words are comma-separated in each row.\n",
    "    Returns a list of all possible word pairs for each row.\n",
    "    \"\"\"\n",
    "    word_pairs = []\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            # Generate all possible word pairs from each row\n",
    "            pairs = list(combinations(row, 2))\n",
    "            word_pairs.extend(pairs)\n",
    "\n",
    "    return word_pairs\n",
    "\n",
    "# Step 2: Process the input CSV file to extract posts and clean the text\n",
    "def process_input_file(input_filename):\n",
    "    \"\"\"\n",
    "    Processes the input CSV file to extract and clean posts. Each post is tokenized,\n",
    "    cleaned of punctuation and stopwords, and stored in a list.\n",
    "    \"\"\"\n",
    "    posts = []\n",
    "    global file_length\n",
    "    df = pd.read_csv(input_filename)  # Load the CSV file into a DataFrame\n",
    "\n",
    "    # Assuming 'comments' is the column that contains the text\n",
    "    for index, row in df.iterrows():\n",
    "        cleaned_post = clean_text(row['Review'])  # Clean and tokenize the post\n",
    "        posts.append(cleaned_post)\n",
    "\n",
    "    file_length = len(df)  # Get the total number of rows\n",
    "    return posts\n",
    "\n",
    "# Step 3: Calculate word frequencies and word pair co-occurrences (distance ≥ 5 words)\n",
    "def calculate_frequencies(posts):\n",
    "    \"\"\"\n",
    "    Calculates the frequency of individual words and word pairs within the posts.\n",
    "    Updates the global word_frequency and word_pair_frequency dictionaries.\n",
    "    Only considers word pairs that are 5 or more words apart.\n",
    "    \"\"\"\n",
    "    global word_frequency, word_pair_frequency\n",
    "\n",
    "    for post in posts:\n",
    "        word_positions = {}  # Dictionary to track positions of each word\n",
    "\n",
    "        # Track word positions\n",
    "        for idx, word in enumerate(post):\n",
    "            if word not in word_positions:\n",
    "                word_positions[word] = []\n",
    "            word_positions[word].append(idx)\n",
    "\n",
    "        # Count word frequencies\n",
    "        unique_words = set(post)  # Track unique words in the post to avoid double counting\n",
    "        for word in unique_words:\n",
    "            word_frequency[word] = word_frequency.get(word, 0) + 1\n",
    "\n",
    "        # Track word pairs that have already been marked as co-occurring in this post\n",
    "        seen_pairs = set()\n",
    "        # Count word pair co-occurrences with distance check\n",
    "        for word1 in word_positions:\n",
    "            for word2 in word_positions:\n",
    "                if word1 != word2 and (word1, word2) not in seen_pairs:\n",
    "                    # Check if the words are 5 or fewer positions apart\n",
    "                    found_pair = False\n",
    "                    for pos1 in word_positions[word1]:\n",
    "                        for pos2 in word_positions[word2]:\n",
    "                            if abs(pos1 - pos2) <= 7:\n",
    "                                word_pair_frequency[word1][word2] = word_pair_frequency.get(word1, {}).get(word2, 0) + 1\n",
    "                                seen_pairs.add((word1, word2))  # Mark this pair as seen\n",
    "                                found_pair = True\n",
    "                                break  # No need to check more positions; move to the next pair\n",
    "                        if found_pair:\n",
    "                            break  # Stop after finding one valid pair in this post\n",
    "                            \n",
    "# Step 4: Calculate the lift between word pairs\n",
    "def calculate_lift(word_pairs):\n",
    "    \"\"\"\n",
    "    Calculates the lift between word pairs using the formula:\n",
    "    Lift(word1, word2) = P(word1 AND word2) / (P(word1) * P(word2))\n",
    "    Lift is written to the lift values CSV and stored in a DataFrame for further processing.\n",
    "    \"\"\"\n",
    "    global itr\n",
    "    \n",
    "    for word1, word2 in word_pairs:\n",
    "        # Get the frequency of word1, word2, and their co-occurrence\n",
    "        freq_word1 = word_frequency.get(word1, 0)\n",
    "        freq_word2 = word_frequency.get(word2, 0)\n",
    "        co_occurrence = word_pair_frequency.get(word1, {}).get(word2, 0)\n",
    "\n",
    "        # Calculate probabilities\n",
    "        p_word1 = freq_word1 / file_length if freq_word1 else 0\n",
    "        p_word2 = freq_word2 / file_length if freq_word2 else 0\n",
    "        p_word1_and_word2 = co_occurrence / file_length if co_occurrence else 0\n",
    "\n",
    "        # Avoid division by zero\n",
    "        if p_word1 > 0 and p_word2 > 0:\n",
    "            lift_value = p_word1_and_word2 / (p_word1 * p_word2) if (p_word1 * p_word2) > 0 else 0\n",
    "        else:\n",
    "            lift_value = 0\n",
    "        # Store lift value in DataFrame\n",
    "        df_lift.loc[itr] = [word1, word2, lift_value]\n",
    "        itr += 1\n",
    "    return df_lift\n",
    "\n",
    "# Step 5: Write lift values and matrix to CSV\n",
    "def save_results(df_lift):\n",
    "    \"\"\"\n",
    "    Writes the calculated lift values to a CSV file and also generates a lift matrix,\n",
    "    saving it to another CSV.\n",
    "    \"\"\"\n",
    "    # Save lift values DataFrame to CSV\n",
    "    # must create duplicate word pairs to create 10x10 matrix\n",
    "    df_lift2 = pd.DataFrame({'word1':df_lift.word2, 'word2':df_lift.word1, 'lift_value':df_lift.lift_value})\n",
    "    df_lift = pd.concat([df_lift, df_lift2], ignore_index=True)\n",
    "    df_lift.to_csv(output_lift_values, index=False)\n",
    "\n",
    "    # Generate lift matrix\n",
    "    lift_matrix = pd.pivot_table(df_lift, values='lift_value', index='word1', columns='word2', fill_value=0)\n",
    "    lift_matrix.index.name = ''\n",
    "    lift_matrix.to_csv(output_lift_matrix)\n",
    "\n",
    "# Main function to run all steps\n",
    "def main():\n",
    "    # Step 1: Load word pairs\n",
    "    word_pairs = load_word_pairs(pair_keys_file)\n",
    "\n",
    "    # Step 2: Process the input file to extract posts\n",
    "    posts = process_input_file(input_file)\n",
    "\n",
    "    # Step 3: Calculate frequencies\n",
    "    calculate_frequencies(posts)\n",
    "\n",
    "    # Step 4: Calculate lift values\n",
    "    df_lift = calculate_lift(word_pairs)\n",
    "\n",
    "    # Step 5: Save results\n",
    "    save_results(df_lift)\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>chocolate</th>\n",
       "      <th>dark</th>\n",
       "      <th>vanilla</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chocolate</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.416647</td>\n",
       "      <td>1.969152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dark</td>\n",
       "      <td>1.416647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.180907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vanilla</td>\n",
       "      <td>1.969152</td>\n",
       "      <td>1.180907</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0  chocolate      dark   vanilla\n",
       "0  chocolate   0.000000  1.416647  1.969152\n",
       "1       dark   1.416647  0.000000  1.180907\n",
       "2    vanilla   1.969152  1.180907  0.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/Lift_Matrix.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our customer has said that chocolate, dark, and vanilla are important attributes to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we want to create some alternative profiles too?\n",
    "Attribute preference ranking?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task C: Similarity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to stem and lemmatize our corpus before performing cosine similarities in order to improve the predictive power of our model. These techniques will increase the co-occurrence of words and likely increase the number of reviews associated with our customer's chosen attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Beer  \\\n",
      "0  Kentucky Brunch Brand Stout   \n",
      "1  Kentucky Brunch Brand Stout   \n",
      "2  Kentucky Brunch Brand Stout   \n",
      "3  Kentucky Brunch Brand Stout   \n",
      "4  Kentucky Brunch Brand Stout   \n",
      "\n",
      "                                              Review  similarity_score  \n",
      "0  sampled brewery bottle version beer pours visc...          0.174078  \n",
      "1  perfect barrel aged stout overly sweet nice ba...          0.000000  \n",
      "2  flirtation maple come crescendo toppling golia...          0.238479  \n",
      "3  flirtation maple come crescendo toppling golia...          0.238479  \n",
      "4  tap tg part kbbs release day rating version pe...          0.000000  \n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Initializing stemmer and Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Preprocessing the text (incorporating stemming and lemmatization)\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Removing the stop words and applying both stemming and lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# We are lemmatizing words, but not stemming them; stemming will produce words that have no meaning on their own and will make our analysis more challenging\n",
    "\n",
    "# Loading in the scraped beer reviews \n",
    "beer_reviews = pd.read_csv('data/reduced_length.csv')\n",
    "\n",
    "# Applyin text preprocessing to all reviews\n",
    "beer_reviews['Review'] = beer_reviews['Review'].apply(preprocess_text)\n",
    "\n",
    "# The 3 determined important attributes from Part B\n",
    "attributes = [\"dark\", \"chocolate\", \"vanilla\"]\n",
    "\n",
    "# Preprocessing the attributes\n",
    "attributes_str = preprocess_text(' '.join(attributes))\n",
    "\n",
    "# Initializing the Bag-of-Words Model\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit transforming both the reviews and the attributes\n",
    "review_vectors = vectorizer.fit_transform(beer_reviews['Review'])\n",
    "attributes_vector = vectorizer.transform([attributes_str])\n",
    "\n",
    "\n",
    "def normalize(vector):\n",
    "    norm = np.linalg.norm(vector)\n",
    "    if norm == 0:  # Prevent division by zero\n",
    "        return vector\n",
    "    return vector / norm\n",
    "\n",
    "normalized_data = []\n",
    "row_indices = []\n",
    "col_indices = []\n",
    "\n",
    "for i in range(review_vectors.shape[0]):\n",
    "    row = review_vectors[i].toarray().flatten()  # Convert sparse row to a dense array\n",
    "    normalized_row = normalize(row)  # Normalize the row\n",
    "    \n",
    "    # Get the non-zero elements and their column indices after normalization\n",
    "    non_zero_cols = np.nonzero(normalized_row)[0]\n",
    "    non_zero_values = normalized_row[non_zero_cols]\n",
    "    \n",
    "    # Store the normalized data for constructing a new sparse matrix\n",
    "    normalized_data.extend(non_zero_values)\n",
    "    row_indices.extend([i] * len(non_zero_values))\n",
    "    col_indices.extend(non_zero_cols)\n",
    "\n",
    "# Create a new sparse matrix using the normalized data\n",
    "review_vectors = csr_matrix((normalized_data, (row_indices, col_indices)),\n",
    "                                      shape=review_vectors.shape)\n",
    "\n",
    "normalized_data = []\n",
    "row_indices = []\n",
    "col_indices = []\n",
    "\n",
    "for i in range(attributes_vector.shape[0]):\n",
    "    row = attributes_vector[i].toarray().flatten()  # Convert sparse row to a dense array\n",
    "    normalized_row = normalize(row)  # Normalize the row\n",
    "    \n",
    "    # Get the non-zero elements and their column indices after normalization\n",
    "    non_zero_cols = np.nonzero(normalized_row)[0]\n",
    "    non_zero_values = normalized_row[non_zero_cols]\n",
    "    \n",
    "    # Store the normalized data for constructing a new sparse matrix\n",
    "    normalized_data.extend(non_zero_values)\n",
    "    row_indices.extend([i] * len(non_zero_values))\n",
    "    col_indices.extend(non_zero_cols)\n",
    "\n",
    "# Create a new sparse matrix using the normalized data\n",
    "attributes_vector = csr_matrix((normalized_data, (row_indices, col_indices)),\n",
    "                                      shape=attributes_vector.shape)\n",
    "\n",
    "\n",
    "# Calculating the cosine similarity between each review and the 3 important attributes\n",
    "sim_scores = cosine_similarity(review_vectors, attributes_vector).flatten()\n",
    "\n",
    "# Adding the similarity scores as a column to the DataFrame\n",
    "beer_reviews['similarity_score'] = sim_scores\n",
    "\n",
    "# Storing the results as a DataFrame\n",
    "sim_scores_df = beer_reviews[['Beer', 'Review', 'similarity_score']]\n",
    "\n",
    "# Saving the results to a CSV output file\n",
    "sim_scores_df.to_csv('data/similarity_scores.csv', index=False)\n",
    "\n",
    "# Printing out the first 5 results (for visualization)\n",
    "print(sim_scores_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task D: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/fmunting/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Beer  Rating  \\\n",
      "0  Kentucky Brunch Brand Stout    4.61   \n",
      "1  Kentucky Brunch Brand Stout    4.71   \n",
      "2  Kentucky Brunch Brand Stout    5.00   \n",
      "3  Kentucky Brunch Brand Stout    4.80   \n",
      "4  Kentucky Brunch Brand Stout    4.98   \n",
      "\n",
      "                                              Review    neg    neu    pos  \\\n",
      "0  Sampled at the brewery, this is the 2022 bottl...  0.028  0.795  0.177   \n",
      "1  The perfect barrel aged stout. Not overly swee...  0.205  0.553  0.242   \n",
      "2  The flirtation with maple comes to a crescendo...  0.024  0.793  0.183   \n",
      "3  The flirtation with maple comes to a crescendo...  0.024  0.793  0.183   \n",
      "4  On tap at TG for part of KBBS release day - ra...  0.077  0.610  0.313   \n",
      "\n",
      "   compound  \n",
      "0    0.9907  \n",
      "1    0.2216  \n",
      "2    0.9831  \n",
      "3    0.9831  \n",
      "4    0.9616  \n"
     ]
    }
   ],
   "source": [
    "# Load CSV file\n",
    "csvFile = pd.read_csv('data/reduced_length.csv')\n",
    "\n",
    "# Set up the analyzer\n",
    "nltk.download('vader_lexicon')\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Create empty lists to store sentiment scores\n",
    "neg = []\n",
    "neu = []\n",
    "pos = []\n",
    "compound = []\n",
    "\n",
    "# Loop through the texts and get the sentiment scores for each one\n",
    "for text in csvFile[\"Review\"]:\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    neg.append(scores['neg'])\n",
    "    neu.append(scores['neu'])\n",
    "    pos.append(scores['pos'])\n",
    "    compound.append(scores['compound'])\n",
    "\n",
    "sentiments = csvFile\n",
    "\n",
    "# Add sentiment scores as new columns to the DataFrame\n",
    "sentiments['neg'] = neg\n",
    "sentiments['neu'] = neu\n",
    "sentiments['pos'] = pos\n",
    "sentiments['compound'] = compound\n",
    "\n",
    "# Display the updated DataFrame with sentiment scores\n",
    "print(sentiments.head())\n",
    "\n",
    "sentiment_avg = sentiments.groupby([\"Beer\"])[\"compound\"].mean().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task E: Beer Evaluation and Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beer\n",
      "Fundamental Observation                    0.275805\n",
      "Speedway Stout - Bourbon Barrel-Aged       0.231605\n",
      "Imperial German Chocolate Cupcake Stout    0.189704\n",
      "Name: Score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# The score is similarity * sentiment\n",
    "\n",
    "sentiment_avg = sentiments.groupby([\"Beer\"])[\"compound\"].mean().sort_values(ascending = False)\n",
    "\n",
    "sentiment_scores = sentiments[[\"Beer\", \"compound\"]]\n",
    "sim_scores = sim_scores_df[[\"Beer\", \"similarity_score\"]]\n",
    "\n",
    "scores = pd.DataFrame()\n",
    "scores[\"Beer\"] = sentiment_scores[\"Beer\"]\n",
    "scores[\"Score\"] = sentiment_scores[\"compound\"] * sim_scores[\"similarity_score\"]\n",
    "print(scores.groupby([\"Beer\"])[\"Score\"].mean().sort_values(ascending = False)[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above are the three beers we would recommend to our customer who likes dark, chocolate and vanilla in their beers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task F: Beer Recommendation Using Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fmunting/opt/anaconda3/lib/python3.9/site-packages/thinc/compat.py:36: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  hasattr(torch, \"has_mps\")\n",
      "/Users/fmunting/opt/anaconda3/lib/python3.9/site-packages/thinc/compat.py:37: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  and torch.has_mps  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_md'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/Users/fmunting/Desktop/UT_MSBA/Fall_Semester/Unstructured_Data/Assignment 2/mis284n_assignment_2/assignment2.ipynb Cell 33\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fmunting/Desktop/UT_MSBA/Fall_Semester/Unstructured_Data/Assignment%202/mis284n_assignment_2/assignment2.ipynb#X44sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpandas\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fmunting/Desktop/UT_MSBA/Fall_Semester/Unstructured_Data/Assignment%202/mis284n_assignment_2/assignment2.ipynb#X44sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Below: sample code\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/fmunting/Desktop/UT_MSBA/Fall_Semester/Unstructured_Data/Assignment%202/mis284n_assignment_2/assignment2.ipynb#X44sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39men_core_web_md\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fmunting/Desktop/UT_MSBA/Fall_Semester/Unstructured_Data/Assignment%202/mis284n_assignment_2/assignment2.ipynb#X44sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m text1 \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mWhat should be done to end violence in the world?\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fmunting/Desktop/UT_MSBA/Fall_Semester/Unstructured_Data/Assignment%202/mis284n_assignment_2/assignment2.ipynb#X44sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m text2 \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mHow can we have peace on earth?\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/__init__.py:54\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[1;32m     31\u001b[0m     name: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[1;32m     32\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     config: Union[Dict[\u001b[39mstr\u001b[39m, Any], Config] \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     38\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Language:\n\u001b[1;32m     39\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[39m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39;49mload_model(\n\u001b[1;32m     55\u001b[0m         name,\n\u001b[1;32m     56\u001b[0m         vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[1;32m     57\u001b[0m         disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[1;32m     58\u001b[0m         enable\u001b[39m=\u001b[39;49menable,\n\u001b[1;32m     59\u001b[0m         exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[1;32m     60\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m     61\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/util.py:439\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    438\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE941\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname, full\u001b[39m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[39m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 439\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE050\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_md'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pandas\n",
    "\n",
    "# Below: sample code\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "text1 = 'What should be done to end violence in the world?'\n",
    "text2 = 'How can we have peace on earth?'\n",
    "doc1 = nlp(text1) # convert text to vector representation\n",
    "doc2 = nlp(text2) # convert text to vector representation\n",
    "\n",
    "attribute_doc = nlp(\"dark, chocolate, vanilla\")\n",
    "\n",
    "spacy_similarities = []\n",
    "\n",
    "df = pd.read_csv(\"data/reduced_length.csv\")\n",
    "\n",
    "for i in range(len(df)):\n",
    "    review = nlp(df.iloc[i, \"Review\"])\n",
    "    spacy_similarities.append(review.similarity(attribute_doc))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task G: Highly Rated Beers\n",
    "Compare the attributes of highly rated beers to what the customer asked for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do people talk about the attributes our customer cares about when writing about these top three beers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Beer\n",
       "M.J.K.                                     4.843333\n",
       "Kentucky Brunch Brand Stout                4.828571\n",
       "A Deal With The Devil - Triple Oak-Aged    4.798000\n",
       "Name: Rating, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/reduced_length.csv\")\n",
    "average_ratings = df.groupby([\"Beer\"])[\"Rating\"].mean().sort_values(ascending = False)\n",
    "average_ratings.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER THIS QUESTION: how often are our attributes of interest mentioned?\n",
    "\n",
    "BONUS: what attributes are actually mentioned in these highly rated beers? I.e. if you like these things you can just get the highly rated beers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word frequencies written to data/top_beer_word_freq.csv\n"
     ]
    }
   ],
   "source": [
    "top_three_beers = [\"M.J.K.\", \"Kentucky Brunch Brand Stout\", \"A Deal With The Devil - Triple Oak-Aged\"]\n",
    "top_beer_reviews = df[df[\"Beer\"].isin(top_three_beers)]\n",
    "\n",
    "top_beer_reviews.to_csv(\"data/top_beer_reviews.csv\", index = False)\n",
    "\n",
    "input_filename = \"data/top_beer_reviews.csv\"\n",
    "word_freq_output = \"data/top_beer_word_freq.csv\"\n",
    "\n",
    "# Main function to run all steps\n",
    "def main():\n",
    "    \n",
    "    # Step 1: Extract and clean sentences\n",
    "    sentences = extract_sentences(input_filename)\n",
    "    \n",
    "    # Step 2: Calculate word frequencies\n",
    "    word_freq = calculate_word_frequencies(sentences)\n",
    "    \n",
    "    # Step 3: Write word frequencies to CSV\n",
    "    write_word_frequencies(word_freq, word_freq_output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# note: these are the raw word frequencies. Does it make sense to do this instead of post count since we have many fewer reviews\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task H: Similar Beers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choose a set of 5 beers\n",
    "\n",
    "pick one and find which of the other 4 is most similar\n",
    "\n",
    "Note: Make it 5 attributes, 5 beers instead of 10 for each\n",
    "- he didn't mention anything about # of attributes in the project overview. Does he want us to manually find the top attributes for each beer? \n",
    "\n",
    "Methodology: pick 5 beers, use spacy to compare all of the reviews to the reviews of the other reviews. Do we combine all of the reviews into one corpus (one long review)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f3c82d000bb8a1750346fb87c49af9771012dc5d265e5e793ee442432926adf7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
