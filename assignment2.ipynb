{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics for Unstructured Data Assignment 2\n",
    "## Building a Crowdsourced Recommender System\n",
    "\n",
    "Team members:\n",
    "- Marcus Martinez\n",
    "- Marifer Martinez-Garcia\n",
    "- Femke Munting\n",
    "- Alex Schmelzeis\n",
    "- Milan Vaghani\n",
    "- Kennedy Zapalac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/Users/fmunting/opt/anaconda3/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Users/fmunting/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/fmunting/opt/anaconda3/lib/python3.9/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/fmunting/opt/anaconda3/lib/python3.9/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/fmunting/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/fmunting/opt/anaconda3/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/fmunting/opt/anaconda3/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/fmunting/opt/anaconda3/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/fmunting/opt/anaconda3/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/fmunting/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/fmunting/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 460, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/fmunting/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 367, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/fmunting/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 662, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/fmunting/opt/anaconda3/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 360, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/Users/fmunting/opt/anaconda3/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/fmunting/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2863, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/fmunting/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2909, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/Users/fmunting/opt/anaconda3/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/fmunting/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3106, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/fmunting/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3309, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/fmunting/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3369, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/gs/rvxrkqns1ws3722g5ty7jpqw0000gn/T/ipykernel_86963/2372813864.py\", line 34, in <cell line: 34>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"/Users/fmunting/opt/anaconda3/lib/python3.9/site-packages/matplotlib/__init__.py\", line 109, in <module>\n",
      "    from . import _api, _version, cbook, docstring, rcsetup\n",
      "  File \"/Users/fmunting/opt/anaconda3/lib/python3.9/site-packages/matplotlib/rcsetup.py\", line 27, in <module>\n",
      "    from matplotlib.colors import Colormap, is_color_like\n",
      "  File \"/Users/fmunting/opt/anaconda3/lib/python3.9/site-packages/matplotlib/colors.py\", line 56, in <module>\n",
      "    from matplotlib import _api, cbook, scale\n",
      "  File \"/Users/fmunting/opt/anaconda3/lib/python3.9/site-packages/matplotlib/scale.py\", line 23, in <module>\n",
      "    from matplotlib.ticker import (\n",
      "  File \"/Users/fmunting/opt/anaconda3/lib/python3.9/site-packages/matplotlib/ticker.py\", line 136, in <module>\n",
      "    from matplotlib import transforms as mtransforms\n",
      "  File \"/Users/fmunting/opt/anaconda3/lib/python3.9/site-packages/matplotlib/transforms.py\", line 46, in <module>\n",
      "    from matplotlib._path import (\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/fmunting/Desktop/UT_MSBA/Fall_Semester/Unstructured_Data/Assignment 2/mis284n_assignment_2/assignment2.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fmunting/Desktop/UT_MSBA/Fall_Semester/Unstructured_Data/Assignment%202/mis284n_assignment_2/assignment2.ipynb#W1sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fmunting/Desktop/UT_MSBA/Fall_Semester/Unstructured_Data/Assignment%202/mis284n_assignment_2/assignment2.ipynb#W1sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# Data visualization\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/fmunting/Desktop/UT_MSBA/Fall_Semester/Unstructured_Data/Assignment%202/mis284n_assignment_2/assignment2.ipynb#W1sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fmunting/Desktop/UT_MSBA/Fall_Semester/Unstructured_Data/Assignment%202/mis284n_assignment_2/assignment2.ipynb#W1sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fmunting/Desktop/UT_MSBA/Fall_Semester/Unstructured_Data/Assignment%202/mis284n_assignment_2/assignment2.ipynb#W1sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# Machine learning and manifold learning\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/matplotlib/__init__.py:109\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpackaging\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m parse \u001b[39mas\u001b[39;00m parse_version\n\u001b[1;32m    107\u001b[0m \u001b[39m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _api, _version, cbook, docstring, rcsetup\n\u001b[1;32m    110\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcbook\u001b[39;00m \u001b[39mimport\u001b[39;00m MatplotlibDeprecationWarning, sanitize_sequence\n\u001b[1;32m    111\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcbook\u001b[39;00m \u001b[39mimport\u001b[39;00m mplDeprecation  \u001b[39m# deprecated\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/matplotlib/rcsetup.py:27\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m _api, cbook\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcbook\u001b[39;00m \u001b[39mimport\u001b[39;00m ls_mapper\n\u001b[0;32m---> 27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolors\u001b[39;00m \u001b[39mimport\u001b[39;00m Colormap, is_color_like\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfontconfig_pattern\u001b[39;00m \u001b[39mimport\u001b[39;00m parse_fontconfig_pattern\n\u001b[1;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_enums\u001b[39;00m \u001b[39mimport\u001b[39;00m JoinStyle, CapStyle\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/matplotlib/colors.py:56\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmpl\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m _api, cbook, scale\n\u001b[1;32m     57\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_color_data\u001b[39;00m \u001b[39mimport\u001b[39;00m BASE_COLORS, TABLEAU_COLORS, CSS4_COLORS, XKCD_COLORS\n\u001b[1;32m     60\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39m_ColorMapping\u001b[39;00m(\u001b[39mdict\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/matplotlib/scale.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmpl\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m _api, docstring\n\u001b[0;32m---> 23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mticker\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m     NullFormatter, ScalarFormatter, LogFormatterSciNotation, LogitFormatter,\n\u001b[1;32m     25\u001b[0m     NullLocator, LogLocator, AutoLocator, AutoMinorLocator,\n\u001b[1;32m     26\u001b[0m     SymmetricalLogLocator, LogitLocator)\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mimport\u001b[39;00m Transform, IdentityTransform\n\u001b[1;32m     30\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mScaleBase\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/matplotlib/ticker.py:136\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmpl\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m _api, cbook\n\u001b[0;32m--> 136\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m transforms \u001b[39mas\u001b[39;00m mtransforms\n\u001b[1;32m    138\u001b[0m _log \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    140\u001b[0m __all__ \u001b[39m=\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mTickHelper\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mFormatter\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mFixedFormatter\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    141\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mNullFormatter\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mFuncFormatter\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mFormatStrFormatter\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    142\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mStrMethodFormatter\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mScalarFormatter\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLogFormatter\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mMultipleLocator\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMaxNLocator\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAutoMinorLocator\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    149\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mSymmetricalLogLocator\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLogitLocator\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/matplotlib/transforms.py:46\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinalg\u001b[39;00m \u001b[39mimport\u001b[39;00m inv\n\u001b[1;32m     45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m _api\n\u001b[0;32m---> 46\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_path\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     47\u001b[0m     affine_transform, count_bboxes_overlapping_bbox, update_path_extents)\n\u001b[1;32m     48\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpath\u001b[39;00m \u001b[39mimport\u001b[39;00m Path\n\u001b[1;32m     50\u001b[0m DEBUG \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "# # pip install undected_chromedriver #run this if you are using it for the first time\n",
    "# Importing modules related to web automation (Selenium)\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "# Uncomment the following if using undetected_chromedriver for scraping\n",
    "# import undetected_chromedriver as uc\n",
    "\n",
    "# Importing standard libraries\n",
    "import time\n",
    "import string\n",
    "import io\n",
    "import csv\n",
    "import re\n",
    "import shutil\n",
    "import random\n",
    "from tempfile import NamedTemporaryFile\n",
    "import decimal\n",
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "from itertools import combinations\n",
    "import operator\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning and manifold learning\n",
    "from sklearn import manifold\n",
    "\n",
    "# Natural language processing (NLP)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Machine learning and text processing\n",
    "from sklearn import manifold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping\n",
    "We have commented out the scraping code for now, but you can run it if you remove the comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for Beer Advocate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the undetected Chrome driver with options\n",
    "# options = uc.ChromeOptions()\n",
    "# options.add_argument('--no-sandbox')\n",
    "# options.add_argument('--disable-dev-shm-usage')\n",
    "# options.add_argument('--window-size=1920,1080')\n",
    "# options.add_argument('--disable-blink-features=AutomationControlled')  # Helps avoid detection\n",
    "\n",
    "# # Initialize the Chrome WebDriver using undetected-chromedriver\n",
    "# driver = uc.Chrome(options=options)\n",
    "\n",
    "# # Open the BeerAdvocate top-rated beers page\n",
    "# driver.get('https://www.beeradvocate.com/beer/top-rated/')\n",
    "\n",
    "# wait = WebDriverWait(driver, 10)\n",
    "\n",
    "# # Dictionary to store beer data\n",
    "# beer_data = {}\n",
    "\n",
    "# # Loop through each beer listed on the page (250 beers)\n",
    "# for i in range(2, 252):  # Beer list starts from row 2 and goes up to 251\n",
    "#     try:\n",
    "#         # Locate the beer name and URL\n",
    "#         beer_link = wait.until(EC.presence_of_element_located(\n",
    "#             (By.XPATH, f'//div[@id=\"ba-content\"]/table/tbody/tr[{i}]/td[2]/a')))\n",
    "#         beer_name = beer_link.text\n",
    "#         beer_url = beer_link.get_attribute('href')\n",
    "#         # Store the name and URL in the dictionary\n",
    "#         beer_data[beer_name] = beer_url\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in processing beer {i}: {e}\")\n",
    "#         continue\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the undetected Chrome driver with options\n",
    "# options = uc.ChromeOptions()\n",
    "# options.add_argument('--no-sandbox')\n",
    "# options.add_argument('--disable-dev-shm-usage')\n",
    "# options.add_argument('--window-size=1920,1080')\n",
    "# options.add_argument('--disable-blink-features=AutomationControlled')  # Helps avoid detection\n",
    "\n",
    "# # Initialize the Chrome WebDriver using undetected-chromedriver\n",
    "# driver = uc.Chrome(options=options)\n",
    "# # Open the first beer's page\n",
    "# comments_file = pd.DataFrame( columns = ['Beer','Rating','Review'])\n",
    "\n",
    "# for key,value in beer_data.items():\n",
    "#         driver.get(value)\n",
    "#         reviews = driver.find_elements(By.XPATH,'//div[@id=\"rating_fullview_content_2\"]')\n",
    "#         for review in reviews[:25]:\n",
    "#             try:\n",
    "#                 review_cmmnts = rep5 = review.find_element(By.XPATH,'.//div').text\n",
    "#                 rating = review.find_element(By.XPATH,'.//span[2]').text\n",
    "#                 review_cmmnts = review_cmmnts.replace('rDev',\"\")\n",
    "#                 k = pd.DataFrame({'Beer': [key], 'Rating': [rating], 'Review': [review_cmmnts]})\n",
    "#                 comments_file = pd.concat([comments_file, k], ignore_index=True)\n",
    "#             except:\n",
    "#                 pass\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for Rate Beer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize undetected Chrome driver with options\n",
    "# options = webdriver.ChromeOptions()\n",
    "# options.add_argument('--no-sandbox')\n",
    "# options.add_argument('--disable-dev-shm-usage')\n",
    "# options.add_argument('--window-size=1920,1080')\n",
    "# options.add_argument('--disable-blink-features=AutomationControlled')  # Helps avoid detection\n",
    "\n",
    "# # Initialize the Chrome WebDriver using undetected-chromedriver\n",
    "# driver = uc.Chrome(options=options)\n",
    "\n",
    "# # Open the BeerAdvocate top-rated beers page\n",
    "# driver.get('https://www.ratebeer.com/top-beers')\n",
    "\n",
    "# wait = WebDriverWait(driver, 10)\n",
    "\n",
    "# # Dictionary to store beer data\n",
    "# beer_data = {}\n",
    "\n",
    "# # Wait for the page to load (use dynamic waiting instead of time.sleep)\n",
    "# time.sleep(5)\n",
    "\n",
    "# # Loop through each beer listed on the page (usually 50-250 beers)\n",
    "# for i in range(1, 51):  # Adjust the range based on how many items are loaded\n",
    "#     try:\n",
    "#         # Locate the beer name and URL using XPath\n",
    "#         beer_link = wait.until(EC.presence_of_element_located(\n",
    "#             (By.XPATH, f'//*[@id=\"root\"]/div[2]/div[2]/div/div/div/div/div/section[2]/div[2]/div/div/div[2]/div[{i}]/div[2]/a/div[1]')))\n",
    "\n",
    "#         # Get the beer name\n",
    "#         beer_name = beer_link.text\n",
    "        \n",
    "#         # Get the beer URL\n",
    "#         beer_url = beer_link.find_element(By.XPATH, '..').get_attribute('href')\n",
    "\n",
    "#         # Store the name and URL in the dictionary\n",
    "#         beer_data[beer_name] = beer_url\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in processing beer {i}: {e}\")\n",
    "#         continue\n",
    "\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the undetected Chrome driver with options\n",
    "# options = uc.ChromeOptions()\n",
    "# options.add_argument('--no-sandbox')\n",
    "# options.add_argument('--disable-dev-shm-usage')\n",
    "# options.add_argument('--window-size=1920,1080')\n",
    "# options.add_argument('--disable-blink-features=AutomationControlled')  # Helps avoid detection\n",
    "\n",
    "# # Initialize the Chrome WebDriver using undetected-chromedriver\n",
    "# driver = uc.Chrome(options=options)\n",
    "\n",
    "# # Create a DataFrame to store beer reviews\n",
    "\n",
    "# # Iterate over the beer_data dictionary\n",
    "# for beer_name, beer_url in beer_data.items():\n",
    "#     driver.get(beer_url)\n",
    "    \n",
    "#     # Wait for the page to load\n",
    "#     time.sleep(2)  # You may adjust this wait time or implement a better waiting strategy\n",
    "\n",
    "#     # Loop through the first 15 reviews\n",
    "#     for i in range(1, 16):\n",
    "#         try:\n",
    "#             # Get the review comment\n",
    "#             review_xpath = f'//*[@id=\"root\"]/div[2]/div[2]/div/div/div/div[2]/div[1]/div[2]/div/div[3]/div[{i}]/div/section[2]/div[1]/div/div[1]/div'\n",
    "#             review_comment = driver.find_element(By.XPATH, review_xpath).text\n",
    "            \n",
    "#             # Get the rating\n",
    "#             rating_xpath = f'//*[@id=\"root\"]/div[2]/div[2]/div/div/div/div[2]/div[1]/div[2]/div/div[3]/div[{i}]/div/section[1]/div[2]/div[2]/div[2]/span[1]'\n",
    "#             rating = driver.find_element(By.XPATH, rating_xpath).text\n",
    "            \n",
    "#             # Store the data in the DataFrame\n",
    "#             k = pd.DataFrame({'Beer': [beer_name], 'Rating': [rating], 'Review': [review_comment]})\n",
    "#             comments_file = pd.concat([comments_file, k], ignore_index=True)\n",
    "\n",
    "#         except Exception as e:\n",
    "#             continue\n",
    "\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments_file.to_csv('data/beer_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'beer_reviews.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/fmunting/Desktop/UT_MSBA/Fall_Semester/Unstructured_Data/Assignment 2/mis284n_assignment_2/assignment2.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/fmunting/Desktop/UT_MSBA/Fall_Semester/Unstructured_Data/Assignment%202/mis284n_assignment_2/assignment2.ipynb#Y114sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39mbeer_reviews.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fmunting/Desktop/UT_MSBA/Fall_Semester/Unstructured_Data/Assignment%202/mis284n_assignment_2/assignment2.ipynb#Y114sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m reviews \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mgroupby([\u001b[39m\"\u001b[39m\u001b[39mBeer\u001b[39m\u001b[39m\"\u001b[39m])[\u001b[39m\"\u001b[39m\u001b[39mReview\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mcount()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fmunting/Desktop/UT_MSBA/Fall_Semester/Unstructured_Data/Assignment%202/mis284n_assignment_2/assignment2.ipynb#Y114sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# The max number of reviews for one beer is 15, the min is 1\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'beer_reviews.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"beer_reviews.csv\")\n",
    "\n",
    "reviews = df.groupby([\"Beer\"])[\"Review\"].count()\n",
    "# The max number of reviews for one beer is 15, the min is 1\n",
    "\n",
    "df.groupby([\"Beer\"])[\"Review\"].count().median()\n",
    "# Mean count is 6; maybe we should cut off all beers that have less than three reviews?\n",
    "\n",
    "# Display frequency of reviews\n",
    "\n",
    "sns.histplot(reviews, binwidth = 1)\n",
    "plt.title(\"Distribution of Review Counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_than_3 = df.groupby([\"Beer\"])[\"Review\"].count()\n",
    "more_than_3 = more_than_3[more_than_3 >= 5]\n",
    "more_than_3 = more_than_3.reset_index()\n",
    "\n",
    "reduced_df = df[df[\"Beer\"].isin(list(more_than_3[\"Beer\"]))]\n",
    "len(reduced_df[\"Beer\"].unique())\n",
    "reduced_df.to_csv(\"data/reduced_length.csv\", index = False)\n",
    "#len(reduced_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task B: Beer Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use raw word frequneices or post frequenices? Used post frequencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fmunting/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word frequencies written to data/word_freq.csv\n"
     ]
    }
   ],
   "source": [
    "# raw word frequencies\n",
    "# Download stopwords from the NLTK package\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Input and output filenames\n",
    "input_filename = 'data/reduced_length.csv'  # Input file\n",
    "final_filename = 'data/beer_reviews2.csv'  # Intermediate file without the column header\n",
    "word_freq_output = 'data/word_freq.csv'  # Output file for word frequencies\n",
    "\n",
    "# Function to clean and tokenize sentences\n",
    "def clean_and_tokenize(sentence):\n",
    "    \"\"\"\n",
    "    Cleans a given sentence by removing punctuation, converting text to lowercase,\n",
    "    and tokenizing the remaining words.\n",
    "    \"\"\"\n",
    "    # Remove punctuation and convert text to lowercase\n",
    "    sentence = re.sub(f'[{re.escape(string.punctuation)}]', '', sentence.lower())\n",
    "\n",
    "    # Tokenize and remove stopwords\n",
    "    return [word for word in sentence.split()]\n",
    "\n",
    "\n",
    "# Step 1: Extract and clean sentences from the text\n",
    "def extract_sentences(file):\n",
    "    \"\"\"\n",
    "    Extracts text data from the third column of the CSV file, splits it into sentences,\n",
    "    and cleans each sentence by removing punctuation and stopwords.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file)\n",
    "    posts = df.iloc[:,2]\n",
    "    sentences = []\n",
    "    sentences_clean = []\n",
    "    for post in posts:\n",
    "        sentences.extend(re.split('[?.!]', post))\n",
    "    for sentence in sentences:\n",
    "        cleaned_tokens = clean_and_tokenize(sentence)\n",
    "        if cleaned_tokens:\n",
    "            sentences_clean.append(cleaned_tokens)\n",
    "    return sentences_clean\n",
    "\n",
    "\n",
    "# Step 2: Calculate word frequencies\n",
    "def calculate_word_frequencies(sentences):\n",
    "    \"\"\"\n",
    "    Calculates the frequency of each word in the given list of cleaned sentences.\n",
    "    \"\"\"\n",
    "    freqs = {}\n",
    "    total_words = 0\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            freqs[word] = freqs.get(word, 0) + 1\n",
    "            total_words += 1\n",
    "    return freqs\n",
    "\n",
    "# Step 3: Write word frequencies to CSV\n",
    "def write_word_frequencies(word_freq, output_file):\n",
    "    \"\"\"\n",
    "    Writes the word frequencies to the specified CSV file.\n",
    "    \"\"\"\n",
    "    word_freq_df = pd.DataFrame(word_freq.items(), columns = [\"Word\", \"Frequency\"])\n",
    "    word_freq_df = word_freq_df.sort_values(by = \"Frequency\", ascending = False)\n",
    "    word_freq_df.to_csv(output_file, index=False)\n",
    "    print(f\"Word frequencies written to {output_file}\")\n",
    "\n",
    "# Main function to run all steps\n",
    "def main():\n",
    "    \n",
    "    # Step 1: Extract and clean sentences\n",
    "    sentences = extract_sentences(input_filename)\n",
    "    \n",
    "    # Step 2: Calculate word frequencies\n",
    "    word_freq = calculate_word_frequencies(sentences)\n",
    "    \n",
    "    # Step 3: Write word frequencies to CSV\n",
    "    write_word_frequencies(word_freq, word_freq_output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fmunting/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word post counts written to data/word_post_count.csv\n"
     ]
    }
   ],
   "source": [
    "# post word frequency\n",
    "# Download stopwords from the NLTK package\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Input and output filenames\n",
    "input_filename = 'data/reduced_length.csv'  # Input file\n",
    "word_freq_output = 'data/word_post_count.csv'  # Output file for word post count\n",
    "\n",
    "# Function to clean text (removing punctuation and stopwords)\n",
    "def clean_text(post):\n",
    "    \"\"\"\n",
    "    Cleans a given post by removing punctuation and stopwords, and converting text to lowercase.\n",
    "    \"\"\"\n",
    "    post = re.sub(f'[{re.escape(string.punctuation)}]', '', post.lower())  # Remove punctuation and convert to lowercase\n",
    "    words = post.split()  # Tokenize the post\n",
    "    words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Step 1: Clean the posts from the CSV\n",
    "def clean_posts(input_file):\n",
    "    \"\"\"\n",
    "    Reads the input CSV file, cleans the posts, and returns a list of cleaned posts.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_file)  # Load CSV\n",
    "    posts = df.iloc[:, 2]  # Extract the third column (posts)\n",
    "    cleaned_posts = posts.apply(clean_text)  # Apply cleaning to each post\n",
    "    return cleaned_posts\n",
    "\n",
    "# Step 2: Use CountVectorizer to count how many posts each word appears in\n",
    "def count_word_occurrences(posts):\n",
    "    \"\"\"\n",
    "    Uses CountVectorizer to count how many posts each word appears in.\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(binary=True)  # binary=True to count post occurrences (not word frequency)\n",
    "    X = vectorizer.fit_transform(posts)  # Transform the posts into a document-term matrix\n",
    "    word_counts = X.toarray().sum(axis=0)  # Sum the binary values to get the number of posts each word appears in\n",
    "    words = vectorizer.get_feature_names_out()  # Get the words from the vocabulary\n",
    "    word_post_count = dict(zip(words, word_counts))  # Map words to their post counts\n",
    "    return word_post_count\n",
    "\n",
    "# Step 3: Write the word counts to a CSV file\n",
    "def write_word_counts_to_csv(word_post_count, output_file):\n",
    "    \"\"\"\n",
    "    Writes the word-post-counts to the specified CSV file.\n",
    "    \"\"\"\n",
    "    word_post_count_df = pd.DataFrame(list(word_post_count.items()), columns=[\"Word\", \"Post_Count\"])\n",
    "    word_post_count_df = word_post_count_df.sort_values(by=\"Post_Count\", ascending=False)\n",
    "    word_post_count_df.to_csv(output_file, index=False)\n",
    "    print(f\"Word post counts written to {output_file}\")\n",
    "\n",
    "# Main function to run all steps\n",
    "def main():\n",
    "    # Step 1: Clean posts\n",
    "    cleaned_posts = clean_posts(input_filename)\n",
    "    \n",
    "    # Step 2: Count how many posts each word appears in\n",
    "    word_post_count = count_word_occurrences(cleaned_posts)\n",
    "    \n",
    "    # Step 3: Write word post counts to CSV\n",
    "    write_word_counts_to_csv(word_post_count, word_freq_output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Post_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>head</td>\n",
       "      <td>855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>taste</td>\n",
       "      <td>608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bottle</td>\n",
       "      <td>545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aroma</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>beer</td>\n",
       "      <td>475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9453</th>\n",
       "      <td>hangen</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9454</th>\n",
       "      <td>hang</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9455</th>\n",
       "      <td>handful</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9456</th>\n",
       "      <td>hamilton</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9457</th>\n",
       "      <td>ярко</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9458 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word  Post_Count\n",
       "0         head         855\n",
       "1        taste         608\n",
       "2       bottle         545\n",
       "3        aroma         513\n",
       "4         beer         475\n",
       "...        ...         ...\n",
       "9453    hangen           1\n",
       "9454      hang           1\n",
       "9455   handful           1\n",
       "9456  hamilton           1\n",
       "9457      ярко           1\n",
       "\n",
       "[9458 rows x 2 columns]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/word_post_count.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Frequent beer qualities:**\n",
    "# NOTE: THIS NEEDS TO BE CHANGED NOW THAT WE HAVE THE NEW INPUT FILE\n",
    "1. Head: Refers to the foam on top of beer after it is poured, considered a quality aspect.\n",
    "2. Aroma?: Refers to the smell or scent of beer, often a key quality evaluated in tasting.\n",
    "3. Sweet\n",
    "4. Dark\n",
    "5. Pours?\n",
    "6. Chocolate\n",
    "7. Black\n",
    "8. Body?: Describes the mouthfeel or weight of the beer (light, medium, or full-bodied). Might be too general\n",
    "9. Vanilla\n",
    "10. Carbonation\n",
    "11. Light\n",
    "12. Smooth\n",
    "13. Fruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check that how attributes are used\n",
    "#df = pd.read_csv('data/beer_reviews.csv')\n",
    "#df_head = df.loc[df['Review'].str.contains(r'\\bhead\\b', case=False), 'Review']\n",
    "#for i in range(len(df_head)):\n",
    "#    print(df.loc[i, 'Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that 3 attributes chosen occur together\n",
    "# Initialize global variables and data structures\n",
    "df_lift = pd.DataFrame(columns=['word1', 'word2', 'lift_value'])  # To store lift values\n",
    "word_frequency = {}  # Dictionary to store word frequency in posts\n",
    "word_pair_frequency = defaultdict(dict)  # Dictionary to store word pair co-occurrence frequency\n",
    "results_dict = {}  # Dictionary to store results with lift values for word pairs\n",
    "file_length = 0  # Number of rows in the input file\n",
    "itr = 0  # Row iterator for the lift DataFrame\n",
    "\n",
    "# File paths\n",
    "input_file = 'data/reduced_length.csv'  # Input data file\n",
    "pair_keys_file = 'data/beer_attributes.txt'  # File containing the words to calculate lift\n",
    "output_lift_values = 'data/Lift_Values.csv'  # Output file for lift values\n",
    "output_lift_matrix = 'data/Lift_Matrix.csv'  # Output file for lift matrix\n",
    "\n",
    "# Load NLTK stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to clean and tokenize text (removes punctuation and stopwords)\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans a given text by removing punctuation, converting it to lowercase,\n",
    "    and tokenizing it, ignoring any stopwords.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    cleaned_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    return cleaned_tokens\n",
    "\n",
    "# Step 1: Load the words from the edmunds_pair_keys.txt file and generate all pairs\n",
    "def load_word_pairs(filename):\n",
    "    \"\"\"\n",
    "    Loads words from a file where words are comma-separated in each row.\n",
    "    Returns a list of all possible word pairs for each row.\n",
    "    \"\"\"\n",
    "    word_pairs = []\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            # Generate all possible word pairs from each row\n",
    "            pairs = list(combinations(row, 2))\n",
    "            word_pairs.extend(pairs)\n",
    "\n",
    "    return word_pairs\n",
    "\n",
    "# Step 2: Process the input CSV file to extract posts and clean the text\n",
    "def process_input_file(input_filename):\n",
    "    \"\"\"\n",
    "    Processes the input CSV file to extract and clean posts. Each post is tokenized,\n",
    "    cleaned of punctuation and stopwords, and stored in a list.\n",
    "    \"\"\"\n",
    "    posts = []\n",
    "    global file_length\n",
    "    df = pd.read_csv(input_filename)  # Load the CSV file into a DataFrame\n",
    "\n",
    "    # Assuming 'comments' is the column that contains the text\n",
    "    for index, row in df.iterrows():\n",
    "        cleaned_post = clean_text(row['Review'])  # Clean and tokenize the post\n",
    "        posts.append(cleaned_post)\n",
    "\n",
    "    file_length = len(df)  # Get the total number of rows\n",
    "    return posts\n",
    "\n",
    "# Step 3: Calculate word frequencies and word pair co-occurrences (distance ≥ 5 words)\n",
    "def calculate_frequencies(posts):\n",
    "    \"\"\"\n",
    "    Calculates the frequency of individual words and word pairs within the posts.\n",
    "    Updates the global word_frequency and word_pair_frequency dictionaries.\n",
    "    Only considers word pairs that are 5 or more words apart.\n",
    "    \"\"\"\n",
    "    global word_frequency, word_pair_frequency\n",
    "\n",
    "    for post in posts:\n",
    "        word_positions = {}  # Dictionary to track positions of each word\n",
    "\n",
    "        # Track word positions\n",
    "        for idx, word in enumerate(post):\n",
    "            if word not in word_positions:\n",
    "                word_positions[word] = []\n",
    "            word_positions[word].append(idx)\n",
    "\n",
    "        # Count word frequencies\n",
    "        unique_words = set(post)  # Track unique words in the post to avoid double counting\n",
    "        for word in unique_words:\n",
    "            word_frequency[word] = word_frequency.get(word, 0) + 1\n",
    "\n",
    "        # Track word pairs that have already been marked as co-occurring in this post\n",
    "        seen_pairs = set()\n",
    "        # Count word pair co-occurrences with distance check\n",
    "        for word1 in word_positions:\n",
    "            for word2 in word_positions:\n",
    "                if word1 != word2 and (word1, word2) not in seen_pairs:\n",
    "                    # Check if the words are 5 or fewer positions apart\n",
    "                    found_pair = False\n",
    "                    for pos1 in word_positions[word1]:\n",
    "                        for pos2 in word_positions[word2]:\n",
    "                            if abs(pos1 - pos2) <= 7:\n",
    "                                word_pair_frequency[word1][word2] = word_pair_frequency.get(word1, {}).get(word2, 0) + 1\n",
    "                                seen_pairs.add((word1, word2))  # Mark this pair as seen\n",
    "                                found_pair = True\n",
    "                                break  # No need to check more positions; move to the next pair\n",
    "                        if found_pair:\n",
    "                            break  # Stop after finding one valid pair in this post\n",
    "                            \n",
    "# Step 4: Calculate the lift between word pairs\n",
    "def calculate_lift(word_pairs):\n",
    "    \"\"\"\n",
    "    Calculates the lift between word pairs using the formula:\n",
    "    Lift(word1, word2) = P(word1 AND word2) / (P(word1) * P(word2))\n",
    "    Lift is written to the lift values CSV and stored in a DataFrame for further processing.\n",
    "    \"\"\"\n",
    "    global itr\n",
    "    \n",
    "    for word1, word2 in word_pairs:\n",
    "        # Get the frequency of word1, word2, and their co-occurrence\n",
    "        freq_word1 = word_frequency.get(word1, 0)\n",
    "        freq_word2 = word_frequency.get(word2, 0)\n",
    "        co_occurrence = word_pair_frequency.get(word1, {}).get(word2, 0)\n",
    "\n",
    "        # Calculate probabilities\n",
    "        p_word1 = freq_word1 / file_length if freq_word1 else 0\n",
    "        p_word2 = freq_word2 / file_length if freq_word2 else 0\n",
    "        p_word1_and_word2 = co_occurrence / file_length if co_occurrence else 0\n",
    "\n",
    "        # Avoid division by zero\n",
    "        if p_word1 > 0 and p_word2 > 0:\n",
    "            lift_value = p_word1_and_word2 / (p_word1 * p_word2) if (p_word1 * p_word2) > 0 else 0\n",
    "        else:\n",
    "            lift_value = 0\n",
    "        # Store lift value in DataFrame\n",
    "        df_lift.loc[itr] = [word1, word2, lift_value]\n",
    "        itr += 1\n",
    "    return df_lift\n",
    "\n",
    "# Step 5: Write lift values and matrix to CSV\n",
    "def save_results(df_lift):\n",
    "    \"\"\"\n",
    "    Writes the calculated lift values to a CSV file and also generates a lift matrix,\n",
    "    saving it to another CSV.\n",
    "    \"\"\"\n",
    "    # Save lift values DataFrame to CSV\n",
    "    # must create duplicate word pairs to create 10x10 matrix\n",
    "    df_lift2 = pd.DataFrame({'word1':df_lift.word2, 'word2':df_lift.word1, 'lift_value':df_lift.lift_value})\n",
    "    df_lift = pd.concat([df_lift, df_lift2], ignore_index=True)\n",
    "    df_lift.to_csv(output_lift_values, index=False)\n",
    "\n",
    "    # Generate lift matrix\n",
    "    lift_matrix = pd.pivot_table(df_lift, values='lift_value', index='word1', columns='word2', fill_value=0)\n",
    "    lift_matrix.index.name = ''\n",
    "    lift_matrix.to_csv(output_lift_matrix)\n",
    "\n",
    "# Main function to run all steps\n",
    "def main():\n",
    "    # Step 1: Load word pairs\n",
    "    word_pairs = load_word_pairs(pair_keys_file)\n",
    "\n",
    "    # Step 2: Process the input file to extract posts\n",
    "    posts = process_input_file(input_file)\n",
    "\n",
    "    # Step 3: Calculate frequencies\n",
    "    calculate_frequencies(posts)\n",
    "\n",
    "    # Step 4: Calculate lift values\n",
    "    df_lift = calculate_lift(word_pairs)\n",
    "\n",
    "    # Step 5: Save results\n",
    "    save_results(df_lift)\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>chocolate</th>\n",
       "      <th>dark</th>\n",
       "      <th>vanilla</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chocolate</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.416647</td>\n",
       "      <td>1.969152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dark</td>\n",
       "      <td>1.416647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.180907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vanilla</td>\n",
       "      <td>1.969152</td>\n",
       "      <td>1.180907</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0  chocolate      dark   vanilla\n",
       "0  chocolate   0.000000  1.416647  1.969152\n",
       "1       dark   1.416647  0.000000  1.180907\n",
       "2    vanilla   1.969152  1.180907  0.000000"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/Lift_Matrix.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our customer has said that chocolate, dark, and vanilla are important attributes to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we want to create some alternative profiles too?\n",
    "Attribute preference ranking?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task C: Similarity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to stem and lemmatize our corpus before performing cosine similarities in order to improve the predictive power of our model. These techniques will increase the co-occurrence of words and likely increase the number of reviews associated with our customer's chosen attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Beer  \\\n",
      "0  Kentucky Brunch Brand Stout   \n",
      "1  Kentucky Brunch Brand Stout   \n",
      "2  Kentucky Brunch Brand Stout   \n",
      "3  Kentucky Brunch Brand Stout   \n",
      "4  Kentucky Brunch Brand Stout   \n",
      "\n",
      "                                              Review  similarity_score  \n",
      "0  sampled brewery bottle version beer pours visc...          0.174078  \n",
      "1  perfect barrel aged stout overly sweet nice ba...          0.000000  \n",
      "2  flirtation maple come crescendo toppling golia...          0.238479  \n",
      "3  flirtation maple come crescendo toppling golia...          0.238479  \n",
      "4  tap tg part kbbs release day rating version pe...          0.000000  \n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Initializing stemmer and Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Preprocessing the text (incorporating stemming and lemmatization)\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Removing the stop words and applying both stemming and lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# We are lemmatizing words, but not stemming them; stemming will produce words that have no meaning on their own and will make our analysis more challenging\n",
    "\n",
    "# Loading in the scraped beer reviews \n",
    "beer_reviews = pd.read_csv('data/reduced_length.csv')\n",
    "\n",
    "# Applyin text preprocessing to all reviews\n",
    "beer_reviews['Review'] = beer_reviews['Review'].apply(preprocess_text)\n",
    "\n",
    "# The 3 determined important attributes from Part B\n",
    "attributes = [\"dark\", \"chocolate\", \"vanilla\"]\n",
    "\n",
    "# Preprocessing the attributes\n",
    "attributes_str = preprocess_text(' '.join(attributes))\n",
    "\n",
    "# Initializing the Bag-of-Words Model\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit transforming both the reviews and the attributes\n",
    "review_vectors = vectorizer.fit_transform(beer_reviews['Review'])\n",
    "attributes_vector = vectorizer.transform([attributes_str])\n",
    "\n",
    "\n",
    "def normalize(vector):\n",
    "    norm = np.linalg.norm(vector)\n",
    "    if norm == 0:  # Prevent division by zero\n",
    "        return vector\n",
    "    return vector / norm\n",
    "\n",
    "normalized_data = []\n",
    "row_indices = []\n",
    "col_indices = []\n",
    "\n",
    "for i in range(review_vectors.shape[0]):\n",
    "    row = review_vectors[i].toarray().flatten()  # Convert sparse row to a dense array\n",
    "    normalized_row = normalize(row)  # Normalize the row\n",
    "    \n",
    "    # Get the non-zero elements and their column indices after normalization\n",
    "    non_zero_cols = np.nonzero(normalized_row)[0]\n",
    "    non_zero_values = normalized_row[non_zero_cols]\n",
    "    \n",
    "    # Store the normalized data for constructing a new sparse matrix\n",
    "    normalized_data.extend(non_zero_values)\n",
    "    row_indices.extend([i] * len(non_zero_values))\n",
    "    col_indices.extend(non_zero_cols)\n",
    "\n",
    "# Create a new sparse matrix using the normalized data\n",
    "review_vectors = csr_matrix((normalized_data, (row_indices, col_indices)),\n",
    "                                      shape=review_vectors.shape)\n",
    "\n",
    "normalized_data = []\n",
    "row_indices = []\n",
    "col_indices = []\n",
    "\n",
    "for i in range(attributes_vector.shape[0]):\n",
    "    row = attributes_vector[i].toarray().flatten()  # Convert sparse row to a dense array\n",
    "    normalized_row = normalize(row)  # Normalize the row\n",
    "    \n",
    "    # Get the non-zero elements and their column indices after normalization\n",
    "    non_zero_cols = np.nonzero(normalized_row)[0]\n",
    "    non_zero_values = normalized_row[non_zero_cols]\n",
    "    \n",
    "    # Store the normalized data for constructing a new sparse matrix\n",
    "    normalized_data.extend(non_zero_values)\n",
    "    row_indices.extend([i] * len(non_zero_values))\n",
    "    col_indices.extend(non_zero_cols)\n",
    "\n",
    "# Create a new sparse matrix using the normalized data\n",
    "attributes_vector = csr_matrix((normalized_data, (row_indices, col_indices)),\n",
    "                                      shape=attributes_vector.shape)\n",
    "\n",
    "\n",
    "# Calculating the cosine similarity between each review and the 3 important attributes\n",
    "sim_scores = cosine_similarity(review_vectors, attributes_vector).flatten()\n",
    "\n",
    "# Adding the similarity scores as a column to the DataFrame\n",
    "beer_reviews['similarity_score'] = sim_scores\n",
    "\n",
    "# Storing the results as a DataFrame\n",
    "sim_scores_df = beer_reviews[['Beer', 'Review', 'similarity_score']]\n",
    "\n",
    "# Saving the results to a CSV output file\n",
    "sim_scores_df.to_csv('similarity_scores.csv', index=False)\n",
    "\n",
    "# Printing out the first 5 results (for visualization)\n",
    "print(sim_scores_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task D: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/fmunting/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Beer  Rating  \\\n",
      "0  Kentucky Brunch Brand Stout    4.61   \n",
      "1  Kentucky Brunch Brand Stout    4.71   \n",
      "2  Kentucky Brunch Brand Stout    5.00   \n",
      "3  Kentucky Brunch Brand Stout    4.80   \n",
      "4  Kentucky Brunch Brand Stout    4.98   \n",
      "\n",
      "                                              Review    neg    neu    pos  \\\n",
      "0  Sampled at the brewery, this is the 2022 bottl...  0.028  0.795  0.177   \n",
      "1  The perfect barrel aged stout. Not overly swee...  0.205  0.553  0.242   \n",
      "2  The flirtation with maple comes to a crescendo...  0.024  0.793  0.183   \n",
      "3  The flirtation with maple comes to a crescendo...  0.024  0.793  0.183   \n",
      "4  On tap at TG for part of KBBS release day - ra...  0.077  0.610  0.313   \n",
      "\n",
      "   compound  \n",
      "0    0.9907  \n",
      "1    0.2216  \n",
      "2    0.9831  \n",
      "3    0.9831  \n",
      "4    0.9616  \n"
     ]
    }
   ],
   "source": [
    "# Load CSV file\n",
    "csvFile = pd.read_csv('data/reduced_length.csv')\n",
    "\n",
    "# Set up the analyzer\n",
    "nltk.download('vader_lexicon')\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Create empty lists to store sentiment scores\n",
    "neg = []\n",
    "neu = []\n",
    "pos = []\n",
    "compound = []\n",
    "\n",
    "# Loop through the texts and get the sentiment scores for each one\n",
    "for text in csvFile[\"Review\"]:\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    neg.append(scores['neg'])\n",
    "    neu.append(scores['neu'])\n",
    "    pos.append(scores['pos'])\n",
    "    compound.append(scores['compound'])\n",
    "\n",
    "sentiments = csvFile\n",
    "\n",
    "# Add sentiment scores as new columns to the DataFrame\n",
    "sentiments['neg'] = neg\n",
    "sentiments['neu'] = neu\n",
    "sentiments['pos'] = pos\n",
    "sentiments['compound'] = compound\n",
    "\n",
    "# Display the updated DataFrame with sentiment scores\n",
    "print(sentiments.head())\n",
    "\n",
    "sentiment_avg = sentiments.groupby([\"Beer\"])[\"compound\"].mean().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task E: Beer Evaluation and Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Beer\n",
       "Fundamental Observation                    0.275805\n",
       "Speedway Stout - Bourbon Barrel-Aged       0.231605\n",
       "Imperial German Chocolate Cupcake Stout    0.189704\n",
       "Name: Score, dtype: float64"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The score is similarity * sentiment\n",
    "\n",
    "sentiment_avg = sentiments.groupby([\"Beer\"])[\"compound\"].mean().sort_values(ascending = False)\n",
    "\n",
    "sentiment_scores = sentiments[[\"Beer\", \"compound\"]]\n",
    "sim_scores = sim_scores_df[[\"Beer\", \"similarity_score\"]]\n",
    "\n",
    "scores = pd.DataFrame()\n",
    "scores[\"Beer\"] = sentiment_scores[\"Beer\"]\n",
    "scores[\"Score\"] = sentiment_scores[\"compound\"] * sim_scores[\"similarity_score\"]\n",
    "print(scores.groupby([\"Beer\"])[\"Score\"].mean().sort_values(ascending = False)[:3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above are the three beers we would recommend to our customer who likes dark, chocolate and vanilla in their beers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task F: Beer Recommendation Using Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_md'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/Users/fmunting/Desktop/UT_MSBA/Fall_Semester/Unstructured_Data/Assignment 2/mis284n_assignment_2/assignment2.ipynb Cell 33\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fmunting/Desktop/UT_MSBA/Fall_Semester/Unstructured_Data/Assignment%202/mis284n_assignment_2/assignment2.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fmunting/Desktop/UT_MSBA/Fall_Semester/Unstructured_Data/Assignment%202/mis284n_assignment_2/assignment2.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Below: sample code\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/fmunting/Desktop/UT_MSBA/Fall_Semester/Unstructured_Data/Assignment%202/mis284n_assignment_2/assignment2.ipynb#X41sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39men_core_web_md\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fmunting/Desktop/UT_MSBA/Fall_Semester/Unstructured_Data/Assignment%202/mis284n_assignment_2/assignment2.ipynb#X41sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m text1 \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mWhat should be done to end violence in the world?\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fmunting/Desktop/UT_MSBA/Fall_Semester/Unstructured_Data/Assignment%202/mis284n_assignment_2/assignment2.ipynb#X41sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m text2 \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mHow can we have peace on earth?\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[1;32m     28\u001b[0m     name: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[1;32m     29\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     config: Union[Dict[\u001b[39mstr\u001b[39m, Any], Config] \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     35\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Language:\n\u001b[1;32m     36\u001b[0m     \u001b[39m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[39m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39;49mload_model(\n\u001b[1;32m     52\u001b[0m         name,\n\u001b[1;32m     53\u001b[0m         vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[1;32m     54\u001b[0m         disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[1;32m     55\u001b[0m         enable\u001b[39m=\u001b[39;49menable,\n\u001b[1;32m     56\u001b[0m         exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[1;32m     57\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m     58\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    471\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE941\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname, full\u001b[39m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[39m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE050\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_md'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Below: sample code\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "text1 = 'What should be done to end violence in the world?'\n",
    "text2 = 'How can we have peace on earth?'\n",
    "doc1 = nlp(text1) # convert text to vector representation\n",
    "doc2 = nlp(text2) # convert text to vector representation\n",
    "\n",
    "attribute_doc = nlp(\"dark, chocolate, vanilla\")\n",
    "\n",
    "spacy_similarities = []\n",
    "\n",
    "df = pd.read_csv(\"data/reduced_length.csv\")\n",
    "\n",
    "for i in range(len(df)):\n",
    "    review = nlp(df.iloc[i, \"Review\"])\n",
    "    spacy_similarities.append(review.similarity(attribute_doc))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task G: Highly Rated Beers\n",
    "Compare the attributes of highly rated beers to what the customer asked for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do people talk about the attributes our customer cares about when writing about these top three beers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Beer\n",
       "M.J.K.                                     4.843333\n",
       "Kentucky Brunch Brand Stout                4.828571\n",
       "A Deal With The Devil - Triple Oak-Aged    4.798000\n",
       "Name: Rating, dtype: float64"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/reduced_length.csv\")\n",
    "average_ratings = df.groupby([\"Beer\"])[\"Rating\"].mean().sort_values(ascending = False)\n",
    "average_ratings.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER THIS QUESTION: how often are our attributes of interest mentioned?\n",
    "\n",
    "BONUS: what attributes are actually mentioned in these highly rated beers? I.e. if you like these things you can just get the highly rated beers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Beer</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Beer, Rating, Review]\n",
       "Index: []"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"Beer\"] == \"Beer:Barrel:Time (Van Winkle 2023)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"Beer\"].unique())\n",
    "# We have 185 unique beers now that we've removed all beers with fewer than 5 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Distribution of Review Counts')"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "reviews = df.groupby([\"Beer\"])[\"Review\"].count()\n",
    "# The max number of reviews for one beer is 15, the min is 1\n",
    "\n",
    "df.groupby([\"Beer\"])[\"Review\"].count().median()\n",
    "# Mean count is 6; maybe we should cut off all beers that have less than three reviews?\n",
    "\n",
    "# Display frequency of reviews; say it has to have this amount of support\n",
    "\n",
    "sns.histplot(reviews, binwidth = 1)\n",
    "plt.title(\"Distribution of Review Counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task H: Similar Beers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# choose a set of 5 beers\n",
    "\n",
    "# pick one and find which of the other 4 is most similar\n",
    "\n",
    "Note: Make it 5 attributes, 5 beers instead of 10 for each"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f3c82d000bb8a1750346fb87c49af9771012dc5d265e5e793ee442432926adf7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
