{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics for Unstructured Data Assignment 2\n",
    "## Building a Crowdsourced Recommender System\n",
    "\n",
    "Team members:\n",
    "- Marcus Martinez\n",
    "- Marifer Martinez-Garcia\n",
    "- Femke Munting\n",
    "- Alex Schmelzeis\n",
    "- Milan Vaghani\n",
    "- Kennedy Zapalac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fmunting/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/Users/fmunting/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "# # pip install undected_chromedriver #run this if you are using it for the first time\n",
    "# Importing modules related to web automation (Selenium)\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "# Uncomment the following if using undetected_chromedriver for scraping\n",
    "# import undetected_chromedriver as uc\n",
    "\n",
    "# Importing standard libraries\n",
    "import time\n",
    "import string\n",
    "import io\n",
    "import csv\n",
    "import re\n",
    "import shutil\n",
    "import random\n",
    "from tempfile import NamedTemporaryFile\n",
    "import decimal\n",
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "from itertools import combinations\n",
    "import operator\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning and manifold learning\n",
    "from sklearn import manifold\n",
    "\n",
    "# Natural language processing (NLP)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Machine learning and text processing\n",
    "from sklearn import manifold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping\n",
    "We have commented out the scraping code for now, but you can run it if you remove the comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for Beer Advocate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the undetected Chrome driver with options\n",
    "# options = uc.ChromeOptions()\n",
    "# options.add_argument('--no-sandbox')\n",
    "# options.add_argument('--disable-dev-shm-usage')\n",
    "# options.add_argument('--window-size=1920,1080')\n",
    "# options.add_argument('--disable-blink-features=AutomationControlled')  # Helps avoid detection\n",
    "\n",
    "# # Initialize the Chrome WebDriver using undetected-chromedriver\n",
    "# driver = uc.Chrome(options=options)\n",
    "\n",
    "# # Open the BeerAdvocate top-rated beers page\n",
    "# driver.get('https://www.beeradvocate.com/beer/top-rated/')\n",
    "\n",
    "# wait = WebDriverWait(driver, 10)\n",
    "\n",
    "# # Dictionary to store beer data\n",
    "# beer_data = {}\n",
    "\n",
    "# # Loop through each beer listed on the page (250 beers)\n",
    "# for i in range(2, 252):  # Beer list starts from row 2 and goes up to 251\n",
    "#     try:\n",
    "#         # Locate the beer name and URL\n",
    "#         beer_link = wait.until(EC.presence_of_element_located(\n",
    "#             (By.XPATH, f'//div[@id=\"ba-content\"]/table/tbody/tr[{i}]/td[2]/a')))\n",
    "#         beer_name = beer_link.text\n",
    "#         beer_url = beer_link.get_attribute('href')\n",
    "#         # Store the name and URL in the dictionary\n",
    "#         beer_data[beer_name] = beer_url\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in processing beer {i}: {e}\")\n",
    "#         continue\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the undetected Chrome driver with options\n",
    "# options = uc.ChromeOptions()\n",
    "# options.add_argument('--no-sandbox')\n",
    "# options.add_argument('--disable-dev-shm-usage')\n",
    "# options.add_argument('--window-size=1920,1080')\n",
    "# options.add_argument('--disable-blink-features=AutomationControlled')  # Helps avoid detection\n",
    "\n",
    "# # Initialize the Chrome WebDriver using undetected-chromedriver\n",
    "# driver = uc.Chrome(options=options)\n",
    "# # Open the first beer's page\n",
    "# comments_file = pd.DataFrame( columns = ['Beer','Rating','Review'])\n",
    "\n",
    "# for key,value in beer_data.items():\n",
    "#         driver.get(value)\n",
    "#         reviews = driver.find_elements(By.XPATH,'//div[@id=\"rating_fullview_content_2\"]')\n",
    "#         for review in reviews[:25]:\n",
    "#             try:\n",
    "#                 review_cmmnts = rep5 = review.find_element(By.XPATH,'.//div').text\n",
    "#                 rating = review.find_element(By.XPATH,'.//span[2]').text\n",
    "#                 review_cmmnts = review_cmmnts.replace('rDev',\"\")\n",
    "#                 k = pd.DataFrame({'Beer': [key], 'Rating': [rating], 'Review': [review_cmmnts]})\n",
    "#                 comments_file = pd.concat([comments_file, k], ignore_index=True)\n",
    "#             except:\n",
    "#                 pass\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for Rate Beer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize undetected Chrome driver with options\n",
    "# options = webdriver.ChromeOptions()\n",
    "# options.add_argument('--no-sandbox')\n",
    "# options.add_argument('--disable-dev-shm-usage')\n",
    "# options.add_argument('--window-size=1920,1080')\n",
    "# options.add_argument('--disable-blink-features=AutomationControlled')  # Helps avoid detection\n",
    "\n",
    "# # Initialize the Chrome WebDriver using undetected-chromedriver\n",
    "# driver = uc.Chrome(options=options)\n",
    "\n",
    "# # Open the BeerAdvocate top-rated beers page\n",
    "# driver.get('https://www.ratebeer.com/top-beers')\n",
    "\n",
    "# wait = WebDriverWait(driver, 10)\n",
    "\n",
    "# # Dictionary to store beer data\n",
    "# beer_data = {}\n",
    "\n",
    "# # Wait for the page to load (use dynamic waiting instead of time.sleep)\n",
    "# time.sleep(5)\n",
    "\n",
    "# # Loop through each beer listed on the page (usually 50-250 beers)\n",
    "# for i in range(1, 51):  # Adjust the range based on how many items are loaded\n",
    "#     try:\n",
    "#         # Locate the beer name and URL using XPath\n",
    "#         beer_link = wait.until(EC.presence_of_element_located(\n",
    "#             (By.XPATH, f'//*[@id=\"root\"]/div[2]/div[2]/div/div/div/div/div/section[2]/div[2]/div/div/div[2]/div[{i}]/div[2]/a/div[1]')))\n",
    "\n",
    "#         # Get the beer name\n",
    "#         beer_name = beer_link.text\n",
    "        \n",
    "#         # Get the beer URL\n",
    "#         beer_url = beer_link.find_element(By.XPATH, '..').get_attribute('href')\n",
    "\n",
    "#         # Store the name and URL in the dictionary\n",
    "#         beer_data[beer_name] = beer_url\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in processing beer {i}: {e}\")\n",
    "#         continue\n",
    "\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the undetected Chrome driver with options\n",
    "# options = uc.ChromeOptions()\n",
    "# options.add_argument('--no-sandbox')\n",
    "# options.add_argument('--disable-dev-shm-usage')\n",
    "# options.add_argument('--window-size=1920,1080')\n",
    "# options.add_argument('--disable-blink-features=AutomationControlled')  # Helps avoid detection\n",
    "\n",
    "# # Initialize the Chrome WebDriver using undetected-chromedriver\n",
    "# driver = uc.Chrome(options=options)\n",
    "\n",
    "# # Create a DataFrame to store beer reviews\n",
    "\n",
    "# # Iterate over the beer_data dictionary\n",
    "# for beer_name, beer_url in beer_data.items():\n",
    "#     driver.get(beer_url)\n",
    "    \n",
    "#     # Wait for the page to load\n",
    "#     time.sleep(2)  # You may adjust this wait time or implement a better waiting strategy\n",
    "\n",
    "#     # Loop through the first 15 reviews\n",
    "#     for i in range(1, 16):\n",
    "#         try:\n",
    "#             # Get the review comment\n",
    "#             review_xpath = f'//*[@id=\"root\"]/div[2]/div[2]/div/div/div/div[2]/div[1]/div[2]/div/div[3]/div[{i}]/div/section[2]/div[1]/div/div[1]/div'\n",
    "#             review_comment = driver.find_element(By.XPATH, review_xpath).text\n",
    "            \n",
    "#             # Get the rating\n",
    "#             rating_xpath = f'//*[@id=\"root\"]/div[2]/div[2]/div/div/div/div[2]/div[1]/div[2]/div/div[3]/div[{i}]/div/section[1]/div[2]/div[2]/div[2]/span[1]'\n",
    "#             rating = driver.find_element(By.XPATH, rating_xpath).text\n",
    "            \n",
    "#             # Store the data in the DataFrame\n",
    "#             k = pd.DataFrame({'Beer': [beer_name], 'Rating': [rating], 'Review': [review_comment]})\n",
    "#             comments_file = pd.concat([comments_file, k], ignore_index=True)\n",
    "\n",
    "#         except Exception as e:\n",
    "#             continue\n",
    "\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments_file.to_csv('data/beer_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task B: Beer Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use raw word frequneices or post frequenices? Used post frequencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fmunting/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word frequencies written to data/word_freq.csv\n"
     ]
    }
   ],
   "source": [
    "# raw word frequencies\n",
    "# Download stopwords from the NLTK package\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Input and output filenames\n",
    "input_filename = 'data/beer_reviews.csv'  # Input file\n",
    "final_filename = 'data/beer_reviews2.csv'  # Intermediate file without the column header\n",
    "word_freq_output = 'data/word_freq.csv'  # Output file for word frequencies\n",
    "\n",
    "# Function to clean and tokenize sentences\n",
    "def clean_and_tokenize(sentence):\n",
    "    \"\"\"\n",
    "    Cleans a given sentence by removing punctuation, converting text to lowercase,\n",
    "    and tokenizing the remaining words.\n",
    "    \"\"\"\n",
    "    # Remove punctuation and convert text to lowercase\n",
    "    sentence = re.sub(f'[{re.escape(string.punctuation)}]', '', sentence.lower())\n",
    "\n",
    "    # Tokenize and remove stopwords\n",
    "    return [word for word in sentence.split()]\n",
    "\n",
    "\n",
    "# Step 1: Extract and clean sentences from the text\n",
    "def extract_sentences(file):\n",
    "    \"\"\"\n",
    "    Extracts text data from the third column of the CSV file, splits it into sentences,\n",
    "    and cleans each sentence by removing punctuation and stopwords.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file)\n",
    "    posts = df.iloc[:,2]\n",
    "    sentences = []\n",
    "    sentences_clean = []\n",
    "    for post in posts:\n",
    "        sentences.extend(re.split('[?.!]', post))\n",
    "    for sentence in sentences:\n",
    "        cleaned_tokens = clean_and_tokenize(sentence)\n",
    "        if cleaned_tokens:\n",
    "            sentences_clean.append(cleaned_tokens)\n",
    "    return sentences_clean\n",
    "\n",
    "\n",
    "# Step 2: Calculate word frequencies\n",
    "def calculate_word_frequencies(sentences):\n",
    "    \"\"\"\n",
    "    Calculates the frequency of each word in the given list of cleaned sentences.\n",
    "    \"\"\"\n",
    "    freqs = {}\n",
    "    total_words = 0\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            freqs[word] = freqs.get(word, 0) + 1\n",
    "            total_words += 1\n",
    "    return freqs\n",
    "\n",
    "# Step 3: Write word frequencies to CSV\n",
    "def write_word_frequencies(word_freq, output_file):\n",
    "    \"\"\"\n",
    "    Writes the word frequencies to the specified CSV file.\n",
    "    \"\"\"\n",
    "    word_freq_df = pd.DataFrame(word_freq.items(), columns = [\"Word\", \"Frequency\"])\n",
    "    word_freq_df = word_freq_df.sort_values(by = \"Frequency\", ascending = False)\n",
    "    word_freq_df.to_csv(output_file, index=False)\n",
    "    print(f\"Word frequencies written to {output_file}\")\n",
    "\n",
    "# Main function to run all steps\n",
    "def main():\n",
    "    \n",
    "    # Step 1: Extract and clean sentences\n",
    "    sentences = extract_sentences(input_filename)\n",
    "    \n",
    "    # Step 2: Calculate word frequencies\n",
    "    word_freq = calculate_word_frequencies(sentences)\n",
    "    \n",
    "    # Step 3: Write word frequencies to CSV\n",
    "    write_word_frequencies(word_freq, word_freq_output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fmunting/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word post counts written to data/word_post_count.csv\n"
     ]
    }
   ],
   "source": [
    "# post word frequency\n",
    "# Download stopwords from the NLTK package\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Input and output filenames\n",
    "input_filename = 'data/beer_reviews.csv'  # Input file\n",
    "word_freq_output = 'data/word_post_count.csv'  # Output file for word post count\n",
    "\n",
    "# Function to clean text (removing punctuation and stopwords)\n",
    "def clean_text(post):\n",
    "    \"\"\"\n",
    "    Cleans a given post by removing punctuation and stopwords, and converting text to lowercase.\n",
    "    \"\"\"\n",
    "    post = re.sub(f'[{re.escape(string.punctuation)}]', '', post.lower())  # Remove punctuation and convert to lowercase\n",
    "    words = post.split()  # Tokenize the post\n",
    "    words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Step 1: Clean the posts from the CSV\n",
    "def clean_posts(input_file):\n",
    "    \"\"\"\n",
    "    Reads the input CSV file, cleans the posts, and returns a list of cleaned posts.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_file)  # Load CSV\n",
    "    posts = df.iloc[:, 2]  # Extract the third column (posts)\n",
    "    cleaned_posts = posts.apply(clean_text)  # Apply cleaning to each post\n",
    "    return cleaned_posts\n",
    "\n",
    "# Step 2: Use CountVectorizer to count how many posts each word appears in\n",
    "def count_word_occurrences(posts):\n",
    "    \"\"\"\n",
    "    Uses CountVectorizer to count how many posts each word appears in.\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(binary=True)  # binary=True to count post occurrences (not word frequency)\n",
    "    X = vectorizer.fit_transform(posts)  # Transform the posts into a document-term matrix\n",
    "    word_counts = X.toarray().sum(axis=0)  # Sum the binary values to get the number of posts each word appears in\n",
    "    words = vectorizer.get_feature_names_out()  # Get the words from the vocabulary\n",
    "    word_post_count = dict(zip(words, word_counts))  # Map words to their post counts\n",
    "    return word_post_count\n",
    "\n",
    "# Step 3: Write the word counts to a CSV file\n",
    "def write_word_counts_to_csv(word_post_count, output_file):\n",
    "    \"\"\"\n",
    "    Writes the word-post-counts to the specified CSV file.\n",
    "    \"\"\"\n",
    "    word_post_count_df = pd.DataFrame(list(word_post_count.items()), columns=[\"Word\", \"Post_Count\"])\n",
    "    word_post_count_df = word_post_count_df.sort_values(by=\"Post_Count\", ascending=False)\n",
    "    word_post_count_df.to_csv(output_file, index=False)\n",
    "    print(f\"Word post counts written to {output_file}\")\n",
    "\n",
    "# Main function to run all steps\n",
    "def main():\n",
    "    # Step 1: Clean posts\n",
    "    cleaned_posts = clean_posts(input_filename)\n",
    "    \n",
    "    # Step 2: Count how many posts each word appears in\n",
    "    word_post_count = count_word_occurrences(cleaned_posts)\n",
    "    \n",
    "    # Step 3: Write word post counts to CSV\n",
    "    write_word_counts_to_csv(word_post_count, word_freq_output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Post_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>head</td>\n",
       "      <td>967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>taste</td>\n",
       "      <td>702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bottle</td>\n",
       "      <td>586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>beer</td>\n",
       "      <td>574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aroma</td>\n",
       "      <td>555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10381</th>\n",
       "      <td>général</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10382</th>\n",
       "      <td>går</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10383</th>\n",
       "      <td>gyümölcsös</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10384</th>\n",
       "      <td>gyümölcsök</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10385</th>\n",
       "      <td>ярко</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10386 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Word  Post_Count\n",
       "0            head         967\n",
       "1           taste         702\n",
       "2          bottle         586\n",
       "3            beer         574\n",
       "4           aroma         555\n",
       "...           ...         ...\n",
       "10381     général           1\n",
       "10382         går           1\n",
       "10383  gyümölcsös           1\n",
       "10384  gyümölcsök           1\n",
       "10385        ярко           1\n",
       "\n",
       "[10386 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/word_post_count.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Frequent beer qualities:**\n",
    "1. Head: Refers to the foam on top of beer after it is poured, considered a quality aspect.\n",
    "2. Aroma?: Refers to the smell or scent of beer, often a key quality evaluated in tasting.\n",
    "3. Sweet\n",
    "4. Dark\n",
    "5. Pours?\n",
    "6. Chocolate\n",
    "7. Black\n",
    "8. Body?: Describes the mouthfeel or weight of the beer (light, medium, or full-bodied). Might be too general\n",
    "9. Vanilla\n",
    "10. Carbonation\n",
    "11. Light\n",
    "12. Smooth\n",
    "13. Fruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check that how attributes are used\n",
    "#df = pd.read_csv('data/beer_reviews.csv')\n",
    "#df_head = df.loc[df['Review'].str.contains(r'\\bhead\\b', case=False), 'Review']\n",
    "#for i in range(len(df_head)):\n",
    "#    print(df.loc[i, 'Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that 3 attributes chosen occur together\n",
    "# Initialize global variables and data structures\n",
    "df_lift = pd.DataFrame(columns=['word1', 'word2', 'lift_value'])  # To store lift values\n",
    "word_frequency = {}  # Dictionary to store word frequency in posts\n",
    "word_pair_frequency = defaultdict(dict)  # Dictionary to store word pair co-occurrence frequency\n",
    "results_dict = {}  # Dictionary to store results with lift values for word pairs\n",
    "file_length = 0  # Number of rows in the input file\n",
    "itr = 0  # Row iterator for the lift DataFrame\n",
    "\n",
    "# File paths\n",
    "input_file = 'data/beer_reviews.csv'  # Input data file\n",
    "pair_keys_file = 'data/beer_attributes.txt'  # File containing the words to calculate lift\n",
    "output_lift_values = 'data/Lift_Values.csv'  # Output file for lift values\n",
    "output_lift_matrix = 'data/Lift_Matrix.csv'  # Output file for lift matrix\n",
    "\n",
    "# Load NLTK stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to clean and tokenize text (removes punctuation and stopwords)\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans a given text by removing punctuation, converting it to lowercase,\n",
    "    and tokenizing it, ignoring any stopwords.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    cleaned_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    return cleaned_tokens\n",
    "\n",
    "# Step 1: Load the words from the edmunds_pair_keys.txt file and generate all pairs\n",
    "def load_word_pairs(filename):\n",
    "    \"\"\"\n",
    "    Loads words from a file where words are comma-separated in each row.\n",
    "    Returns a list of all possible word pairs for each row.\n",
    "    \"\"\"\n",
    "    word_pairs = []\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            # Generate all possible word pairs from each row\n",
    "            pairs = list(combinations(row, 2))\n",
    "            word_pairs.extend(pairs)\n",
    "\n",
    "    return word_pairs\n",
    "\n",
    "# Step 2: Process the input CSV file to extract posts and clean the text\n",
    "def process_input_file(input_filename):\n",
    "    \"\"\"\n",
    "    Processes the input CSV file to extract and clean posts. Each post is tokenized,\n",
    "    cleaned of punctuation and stopwords, and stored in a list.\n",
    "    \"\"\"\n",
    "    posts = []\n",
    "    global file_length\n",
    "    df = pd.read_csv(input_filename)  # Load the CSV file into a DataFrame\n",
    "\n",
    "    # Assuming 'comments' is the column that contains the text\n",
    "    for index, row in df.iterrows():\n",
    "        cleaned_post = clean_text(row['Review'])  # Clean and tokenize the post\n",
    "        posts.append(cleaned_post)\n",
    "\n",
    "    file_length = len(df)  # Get the total number of rows\n",
    "    return posts\n",
    "\n",
    "# Step 3: Calculate word frequencies and word pair co-occurrences (distance ≥ 5 words)\n",
    "def calculate_frequencies(posts):\n",
    "    \"\"\"\n",
    "    Calculates the frequency of individual words and word pairs within the posts.\n",
    "    Updates the global word_frequency and word_pair_frequency dictionaries.\n",
    "    Only considers word pairs that are 5 or more words apart.\n",
    "    \"\"\"\n",
    "    global word_frequency, word_pair_frequency\n",
    "\n",
    "    for post in posts:\n",
    "        word_positions = {}  # Dictionary to track positions of each word\n",
    "\n",
    "        # Track word positions\n",
    "        for idx, word in enumerate(post):\n",
    "            if word not in word_positions:\n",
    "                word_positions[word] = []\n",
    "            word_positions[word].append(idx)\n",
    "\n",
    "        # Count word frequencies\n",
    "        unique_words = set(post)  # Track unique words in the post to avoid double counting\n",
    "        for word in unique_words:\n",
    "            word_frequency[word] = word_frequency.get(word, 0) + 1\n",
    "\n",
    "        # Track word pairs that have already been marked as co-occurring in this post\n",
    "        seen_pairs = set()\n",
    "        # Count word pair co-occurrences with distance check\n",
    "        for word1 in word_positions:\n",
    "            for word2 in word_positions:\n",
    "                if word1 != word2 and (word1, word2) not in seen_pairs:\n",
    "                    # Check if the words are 5 or fewer positions apart\n",
    "                    found_pair = False\n",
    "                    for pos1 in word_positions[word1]:\n",
    "                        for pos2 in word_positions[word2]:\n",
    "                            if abs(pos1 - pos2) <= 7:\n",
    "                                word_pair_frequency[word1][word2] = word_pair_frequency.get(word1, {}).get(word2, 0) + 1\n",
    "                                seen_pairs.add((word1, word2))  # Mark this pair as seen\n",
    "                                found_pair = True\n",
    "                                break  # No need to check more positions; move to the next pair\n",
    "                        if found_pair:\n",
    "                            break  # Stop after finding one valid pair in this post\n",
    "                            \n",
    "# Step 4: Calculate the lift between word pairs\n",
    "def calculate_lift(word_pairs):\n",
    "    \"\"\"\n",
    "    Calculates the lift between word pairs using the formula:\n",
    "    Lift(word1, word2) = P(word1 AND word2) / (P(word1) * P(word2))\n",
    "    Lift is written to the lift values CSV and stored in a DataFrame for further processing.\n",
    "    \"\"\"\n",
    "    global itr\n",
    "    \n",
    "    for word1, word2 in word_pairs:\n",
    "        # Get the frequency of word1, word2, and their co-occurrence\n",
    "        freq_word1 = word_frequency.get(word1, 0)\n",
    "        freq_word2 = word_frequency.get(word2, 0)\n",
    "        co_occurrence = word_pair_frequency.get(word1, {}).get(word2, 0)\n",
    "\n",
    "        # Calculate probabilities\n",
    "        p_word1 = freq_word1 / file_length if freq_word1 else 0\n",
    "        p_word2 = freq_word2 / file_length if freq_word2 else 0\n",
    "        p_word1_and_word2 = co_occurrence / file_length if co_occurrence else 0\n",
    "\n",
    "        # Avoid division by zero\n",
    "        if p_word1 > 0 and p_word2 > 0:\n",
    "            lift_value = p_word1_and_word2 / (p_word1 * p_word2) if (p_word1 * p_word2) > 0 else 0\n",
    "        else:\n",
    "            lift_value = 0\n",
    "        # Store lift value in DataFrame\n",
    "        df_lift.loc[itr] = [word1, word2, lift_value]\n",
    "        itr += 1\n",
    "    return df_lift\n",
    "\n",
    "# Step 5: Write lift values and matrix to CSV\n",
    "def save_results(df_lift):\n",
    "    \"\"\"\n",
    "    Writes the calculated lift values to a CSV file and also generates a lift matrix,\n",
    "    saving it to another CSV.\n",
    "    \"\"\"\n",
    "    # Save lift values DataFrame to CSV\n",
    "    # must create duplicate word pairs to create 10x10 matrix\n",
    "    df_lift2 = pd.DataFrame({'word1':df_lift.word2, 'word2':df_lift.word1, 'lift_value':df_lift.lift_value})\n",
    "    df_lift = pd.concat([df_lift, df_lift2], ignore_index=True)\n",
    "    df_lift.to_csv(output_lift_values, index=False)\n",
    "\n",
    "    # Generate lift matrix\n",
    "    lift_matrix = pd.pivot_table(df_lift, values='lift_value', index='word1', columns='word2', fill_value=0)\n",
    "    lift_matrix.index.name = ''\n",
    "    lift_matrix.to_csv(output_lift_matrix)\n",
    "\n",
    "# Main function to run all steps\n",
    "def main():\n",
    "    # Step 1: Load word pairs\n",
    "    word_pairs = load_word_pairs(pair_keys_file)\n",
    "\n",
    "    # Step 2: Process the input file to extract posts\n",
    "    posts = process_input_file(input_file)\n",
    "\n",
    "    # Step 3: Calculate frequencies\n",
    "    calculate_frequencies(posts)\n",
    "\n",
    "    # Step 4: Calculate lift values\n",
    "    df_lift = calculate_lift(word_pairs)\n",
    "\n",
    "    # Step 5: Save results\n",
    "    save_results(df_lift)\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>chocolate</th>\n",
       "      <th>dark</th>\n",
       "      <th>vanilla</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chocolate</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.460186</td>\n",
       "      <td>2.020803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dark</td>\n",
       "      <td>1.460186</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.151893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vanilla</td>\n",
       "      <td>2.020803</td>\n",
       "      <td>1.151893</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0  chocolate      dark   vanilla\n",
       "0  chocolate   0.000000  1.460186  2.020803\n",
       "1       dark   1.460186  0.000000  1.151893\n",
       "2    vanilla   2.020803  1.151893  0.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/Lift_Matrix.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our customer has said that chocolate, dark, and vanilla are important attributes to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we want to create some alternative profiles too?\n",
    "Attribute preference ranking?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task C: Similarity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to stem and lemmatize our corpus before performing cosine similarities in order to improve the predictive power of our model. These techniques will increase the co-occurrence of words and likely increase the number of reviews associated with our customer's chosen attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Beer  \\\n",
      "0  Kentucky Brunch Brand Stout   \n",
      "1  Kentucky Brunch Brand Stout   \n",
      "2  Kentucky Brunch Brand Stout   \n",
      "3  Kentucky Brunch Brand Stout   \n",
      "4  Kentucky Brunch Brand Stout   \n",
      "\n",
      "                                              Review  similarity_score  \n",
      "0  sampled brewery bottle version beer pours visc...          0.174078  \n",
      "1  perfect barrel aged stout overly sweet nice ba...          0.154303  \n",
      "2  flirtation maple come crescendo toppling golia...          0.317971  \n",
      "3  flirtation maple come crescendo toppling golia...          0.317971  \n",
      "4  tap tg part kbbs release day rating version pe...          0.000000  \n"
     ]
    }
   ],
   "source": [
    "# Initializing stemmer and Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Preprocessing the text (incorporating stemming and lemmatization)\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Removing the stop words and applying both stemming and lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# We are lemmatizing words, but not stemming them; stemming will produce words that have no meaning on their own and will make our analysis more challenging\n",
    "\n",
    "# Loading in the scraped beer reviews \n",
    "beer_reviews = pd.read_csv('data/beer_reviews.csv')\n",
    "\n",
    "# Applyin text preprocessing to all reviews\n",
    "beer_reviews['Review'] = beer_reviews['Review'].apply(preprocess_text)\n",
    "\n",
    "# The 3 determined important attributes from Part B\n",
    "attributes = [\"dark\", \"chocolate\", \"sweet\"]\n",
    "\n",
    "# Preprocessing the attributes\n",
    "attributes_str = preprocess_text(' '.join(attributes))\n",
    "\n",
    "# Initializing the Bag-of-Words Model\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit transforming both the reviews and the attributes\n",
    "review_vectors = vectorizer.fit_transform(beer_reviews['Review'])\n",
    "attributes_vector = vectorizer.transform([attributes_str])\n",
    "\n",
    "# Calculating the cosine similarity between each review and the 3 important attributes\n",
    "sim_scores = cosine_similarity(review_vectors, attributes_vector).flatten()\n",
    "\n",
    "# Adding the similarity scores as a column to the DataFrame\n",
    "beer_reviews['similarity_score'] = sim_scores\n",
    "\n",
    "# Storing the results as a DataFrame\n",
    "sim_scores_df = beer_reviews[['Beer', 'Review', 'similarity_score']]\n",
    "\n",
    "# Saving the results to a CSV output file\n",
    "sim_scores_df.to_csv('similarity_scores.csv', index=False)\n",
    "\n",
    "# Printing out the first 5 results (for visualization)\n",
    "print(sim_scores_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions: Do we need to average the similarity scores for each beer? Yes, but I think we can do this after we get the finalized sentiment score? Will this work with our negative sentiment scores?\n",
    "\n",
    "Do we need to normalize the number of posts per beer? Not if we're going to average all of the reviews\n",
    "Do we want the review or the product score? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task D: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - '/Users/fmunting/nltk_data'\n    - '/Users/fmunting/opt/anaconda3/nltk_data'\n    - '/Users/fmunting/opt/anaconda3/share/nltk_data'\n    - '/Users/fmunting/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/fmunting/Desktop/UT_MSBA/Fall_Semester/Unstructured_Data/Assignment 2/mis284n_assignment_2/assignment2.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fmunting/Desktop/UT_MSBA/Fall_Semester/Unstructured_Data/Assignment%202/mis284n_assignment_2/assignment2.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m csvFile \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mdata/beer_reviews.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fmunting/Desktop/UT_MSBA/Fall_Semester/Unstructured_Data/Assignment%202/mis284n_assignment_2/assignment2.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Set up the analyzer\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/fmunting/Desktop/UT_MSBA/Fall_Semester/Unstructured_Data/Assignment%202/mis284n_assignment_2/assignment2.ipynb#X34sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m analyzer \u001b[39m=\u001b[39m SentimentIntensityAnalyzer()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fmunting/Desktop/UT_MSBA/Fall_Semester/Unstructured_Data/Assignment%202/mis284n_assignment_2/assignment2.ipynb#X34sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Create empty lists to store sentiment scores\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fmunting/Desktop/UT_MSBA/Fall_Semester/Unstructured_Data/Assignment%202/mis284n_assignment_2/assignment2.ipynb#X34sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m neg \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/sentiment/vader.py:340\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer.__init__\u001b[0;34m(self, lexicon_file)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    337\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    338\u001b[0m     lexicon_file\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    339\u001b[0m ):\n\u001b[0;32m--> 340\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlexicon_file \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mload(lexicon_file)\n\u001b[1;32m    341\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlexicon \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_lex_dict()\n\u001b[1;32m    342\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconstants \u001b[39m=\u001b[39m VaderConstants()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[1;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - '/Users/fmunting/nltk_data'\n    - '/Users/fmunting/opt/anaconda3/nltk_data'\n    - '/Users/fmunting/opt/anaconda3/share/nltk_data'\n    - '/Users/fmunting/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Load CSV file\n",
    "csvFile = pd.read_csv('data/beer_reviews.csv')\n",
    "\n",
    "# Set up the analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Create empty lists to store sentiment scores\n",
    "neg = []\n",
    "neu = []\n",
    "pos = []\n",
    "compound = []\n",
    "\n",
    "# Loop through the texts and get the sentiment scores for each one\n",
    "for text in csvFile[\"Review\"]:\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    neg.append(scores['neg'])\n",
    "    neu.append(scores['neu'])\n",
    "    pos.append(scores['pos'])\n",
    "    compound.append(scores['compound'])\n",
    "\n",
    "# Add sentiment scores as new columns to the DataFrame\n",
    "csvFile['neg'] = neg\n",
    "csvFile['neu'] = neu\n",
    "csvFile['pos'] = pos\n",
    "csvFile['compound'] = compound\n",
    "\n",
    "# Display the updated DataFrame with sentiment scores\n",
    "print(csvFile.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task E: Beer Evaluation and Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task F: Beer Recommendation Using Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task G: Highly Rated Beers\n",
    "Compare the attributes of highly rated beers to what the customer asked for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do people talk about the attributes our customer cares about when writing about these top three beers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Beer\n",
       "Popinski - Bourbon Barrel-Aged - Peanut Butter And Marshmallow    4.95\n",
       "10 Year Barleywine                                                4.94\n",
       "Beer:Barrel:Time (Van Winkle 2023)                                4.93\n",
       "Name: Rating, dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/beer_reviews.csv\")\n",
    "average_ratings = df.groupby([\"Beer\"])[\"Rating\"].mean().sort_values(ascending = False)\n",
    "average_ratings.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top three beers are: Popinski - Bourbon Barrel-Aged - Peanut Butter And Marshmallow, 10 Year Barleywine and Beer:Barrel:Time (Van Winkle 2023)\n",
    "    Problem: there are only two ratings for Popinski, 1 for Barleywine and 3 for Beer:Barrel:Time. This doesn't seem like a fair comparison\n",
    "\n",
    "ANSWER THIS QUESTION: how often are our attributes of interest mentioned?\n",
    "\n",
    "BONUS: what attributes are actually mentioned in these highly rated beers? I.e. if you have a high "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Beer</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Beer:Barrel:Time (Van Winkle 2023)</td>\n",
       "      <td>4.85</td>\n",
       "      <td>375ml bottle that is waxed with thick bronze/g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Beer:Barrel:Time (Van Winkle 2023)</td>\n",
       "      <td>5.00</td>\n",
       "      <td>Had this last year it was fantastic. Great sme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Beer:Barrel:Time (Van Winkle 2023)</td>\n",
       "      <td>4.94</td>\n",
       "      <td>I've had the chance to try this 3x now. Just a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Beer  Rating  \\\n",
       "116  Beer:Barrel:Time (Van Winkle 2023)    4.85   \n",
       "117  Beer:Barrel:Time (Van Winkle 2023)    5.00   \n",
       "118  Beer:Barrel:Time (Van Winkle 2023)    4.94   \n",
       "\n",
       "                                                Review  \n",
       "116  375ml bottle that is waxed with thick bronze/g...  \n",
       "117  Had this last year it was fantastic. Great sme...  \n",
       "118  I've had the chance to try this 3x now. Just a...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"Beer\"] == \"Beer:Barrel:Time (Van Winkle 2023)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "280"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"Beer\"].unique())\n",
    "# We have 280 unique beers, but an uneven distribution of review counts for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"Beer\"])[\"Review\"].count().max()\n",
    "# The max number of reviews for one beer is 15, the min is 1\n",
    "\n",
    "df.groupby([\"Beer\"])[\"Review\"].count().median()\n",
    "# Mean count is 6; maybe we should cut off all beers that have less than three reviews?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task H: Similar Beers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# choose a set of 5 beers\n",
    "\n",
    "# pick one and find which of the other 4 is most similar\n",
    "\n",
    "Note: Make it 5 attributes, 5 beers instead of 10 for each"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f3c82d000bb8a1750346fb87c49af9771012dc5d265e5e793ee442432926adf7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
